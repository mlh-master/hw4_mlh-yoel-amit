(base) stu4@triton01:~$ cd HW4_yoel_amit
(base) stu4@triton01:~/HW4_yoel_amit$ conda activate 2ndPaper-t2
(2ndPaper-t2) stu4@triton01:~/HW4_yoel_amit$ python HW4.py
2021-03-04 15:02:39.356128: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-03-04 15:02:39.356156: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-03-04 15:02:42.835798: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-04 15:02:42.841516: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-04 15:02:42.843564: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-04 15:02:42.864094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:5e:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 7.77GiB deviceMemoryBandwidth: 417.23GiB/s
2021-03-04 15:02:42.864198: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-03-04 15:02:42.864299: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2021-03-04 15:02:42.864377: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2021-03-04 15:02:42.866691: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-04 15:02:42.867072: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-04 15:02:42.869558: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-04 15:02:42.869683: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2021-03-04 15:02:42.869764: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2021-03-04 15:02:42.869779: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-03-04 15:02:42.973434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-04 15:02:42.973482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-04 15:02:42.973493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
WARNING:tensorflow:From HW4.py:52: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.

2021-03-04 15:03:08.860479: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-04 15:03:08.861673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:5e:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 7.77GiB deviceMemoryBandwidth: 417.23GiB/s
2021-03-04 15:03:08.861819: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-03-04 15:03:08.861903: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2021-03-04 15:03:08.861971: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2021-03-04 15:03:08.861998: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-04 15:03:08.862018: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-04 15:03:08.862037: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-04 15:03:08.862097: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2021-03-04 15:03:08.862162: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2021-03-04 15:03:08.862175: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-03-04 15:03:08.862479: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-04 15:03:08.862503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-04 15:03:08.862512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      
Model: "RELU_64_batches"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 1024)              0         
_________________________________________________________________
dense (Dense)                (None, 300)               307500    
_________________________________________________________________
Relu1 (Activation)           (None, 300)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 150)               45150     
_________________________________________________________________
Relu2 (Activation)           (None, 150)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 4)                 604       
_________________________________________________________________
activation (Activation)      (None, 4)                 0         
=================================================================
Total params: 353,254
Trainable params: 353,254
Non-trainable params: 0
_________________________________________________________________
2021-03-04 15:03:09.051563: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-04 15:03:09.073700: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz
Epoch 1/25
102/102 - 1s - loss: 1.2837 - accuracy: 0.4262 - val_loss: 1.1623 - val_accuracy: 0.6047
Epoch 2/25
102/102 - 0s - loss: 1.1011 - accuracy: 0.6333 - val_loss: 1.0485 - val_accuracy: 0.6696
Epoch 3/25
102/102 - 0s - loss: 1.0147 - accuracy: 0.6818 - val_loss: 0.9835 - val_accuracy: 0.7078
Epoch 4/25
102/102 - 0s - loss: 0.9543 - accuracy: 0.7125 - val_loss: 0.9303 - val_accuracy: 0.7269
Epoch 5/25
102/102 - 0s - loss: 0.9056 - accuracy: 0.7349 - val_loss: 0.8904 - val_accuracy: 0.7384
Epoch 6/25
102/102 - 0s - loss: 0.8653 - accuracy: 0.7478 - val_loss: 0.8548 - val_accuracy: 0.7512
Epoch 7/25
102/102 - 0s - loss: 0.8313 - accuracy: 0.7598 - val_loss: 0.8243 - val_accuracy: 0.7610
Epoch 8/25
102/102 - 0s - loss: 0.7991 - accuracy: 0.7720 - val_loss: 0.7981 - val_accuracy: 0.7610
Epoch 9/25
102/102 - 0s - loss: 0.7702 - accuracy: 0.7802 - val_loss: 0.7690 - val_accuracy: 0.7731
Epoch 10/25
102/102 - 0s - loss: 0.7445 - accuracy: 0.7884 - val_loss: 0.7454 - val_accuracy: 0.7749
Epoch 11/25
102/102 - 0s - loss: 0.7211 - accuracy: 0.7907 - val_loss: 0.7239 - val_accuracy: 0.7853
Epoch 12/25
102/102 - 0s - loss: 0.6980 - accuracy: 0.7989 - val_loss: 0.7066 - val_accuracy: 0.7859
Epoch 13/25
102/102 - 0s - loss: 0.6782 - accuracy: 0.8009 - val_loss: 0.6860 - val_accuracy: 0.7951
Epoch 14/25
102/102 - 0s - loss: 0.6583 - accuracy: 0.8086 - val_loss: 0.6689 - val_accuracy: 0.7963
Epoch 15/25
102/102 - 0s - loss: 0.6411 - accuracy: 0.8131 - val_loss: 0.6526 - val_accuracy: 0.8038
Epoch 16/25
102/102 - 0s - loss: 0.6250 - accuracy: 0.8142 - val_loss: 0.6380 - val_accuracy: 0.8061
Epoch 17/25
102/102 - 0s - loss: 0.6097 - accuracy: 0.8197 - val_loss: 0.6228 - val_accuracy: 0.8084
Epoch 18/25
102/102 - 0s - loss: 0.5962 - accuracy: 0.8251 - val_loss: 0.6111 - val_accuracy: 0.8102
Epoch 19/25
102/102 - 0s - loss: 0.5825 - accuracy: 0.8281 - val_loss: 0.5984 - val_accuracy: 0.8166
Epoch 20/25
102/102 - 0s - loss: 0.5686 - accuracy: 0.8323 - val_loss: 0.5856 - val_accuracy: 0.8200
Epoch 21/25
102/102 - 0s - loss: 0.5560 - accuracy: 0.8312 - val_loss: 0.5746 - val_accuracy: 0.8247
Epoch 22/25
102/102 - 0s - loss: 0.5446 - accuracy: 0.8373 - val_loss: 0.5633 - val_accuracy: 0.8235
Epoch 23/25
102/102 - 0s - loss: 0.5331 - accuracy: 0.8404 - val_loss: 0.5528 - val_accuracy: 0.8275
Epoch 24/25
102/102 - 0s - loss: 0.5226 - accuracy: 0.8440 - val_loss: 0.5455 - val_accuracy: 0.8264
Epoch 25/25
102/102 - 0s - loss: 0.5128 - accuracy: 0.8479 - val_loss: 0.5349 - val_accuracy: 0.8316
6/6 - 0s - loss: 0.7791 - accuracy: 0.6971


Loss and metrics for RELU model with 64 batches and 25 epochs: 
Test Loss is 0.78 
Test Accuracy is 69.71 %

Model: "TANH"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 300)               307500    
_________________________________________________________________
tanh1 (Activation)           (None, 300)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 150)               45150     
_________________________________________________________________
tanh2 (Activation)           (None, 150)               0         
_________________________________________________________________
dense_5 (Dense)              (None, 4)                 604       
_________________________________________________________________
activation_1 (Activation)    (None, 4)                 0         
=================================================================
Total params: 353,254
Trainable params: 353,254
Non-trainable params: 0
_________________________________________________________________
Epoch 1/25
102/102 - 1s - loss: 1.2619 - accuracy: 0.4541 - val_loss: 1.1382 - val_accuracy: 0.6076
Epoch 2/25
102/102 - 0s - loss: 1.0828 - accuracy: 0.6080 - val_loss: 1.0262 - val_accuracy: 0.6476
Epoch 3/25
102/102 - 0s - loss: 0.9926 - accuracy: 0.6515 - val_loss: 0.9606 - val_accuracy: 0.6875
Epoch 4/25
102/102 - 0s - loss: 0.9355 - accuracy: 0.6821 - val_loss: 0.9168 - val_accuracy: 0.6904
Epoch 5/25
102/102 - 0s - loss: 0.8944 - accuracy: 0.6985 - val_loss: 0.8847 - val_accuracy: 0.7043
Epoch 6/25
102/102 - 0s - loss: 0.8610 - accuracy: 0.7093 - val_loss: 0.8567 - val_accuracy: 0.7106
Epoch 7/25
102/102 - 0s - loss: 0.8348 - accuracy: 0.7209 - val_loss: 0.8348 - val_accuracy: 0.7199
Epoch 8/25
102/102 - 0s - loss: 0.8108 - accuracy: 0.7283 - val_loss: 0.8164 - val_accuracy: 0.7234
Epoch 9/25
102/102 - 0s - loss: 0.7899 - accuracy: 0.7349 - val_loss: 0.7974 - val_accuracy: 0.7315
Epoch 10/25
102/102 - 0s - loss: 0.7713 - accuracy: 0.7428 - val_loss: 0.7802 - val_accuracy: 0.7384
Epoch 11/25
102/102 - 0s - loss: 0.7544 - accuracy: 0.7507 - val_loss: 0.7673 - val_accuracy: 0.7413
Epoch 12/25
102/102 - 0s - loss: 0.7388 - accuracy: 0.7527 - val_loss: 0.7547 - val_accuracy: 0.7483
Epoch 13/25
102/102 - 0s - loss: 0.7245 - accuracy: 0.7590 - val_loss: 0.7443 - val_accuracy: 0.7517
Epoch 14/25
102/102 - 0s - loss: 0.7120 - accuracy: 0.7637 - val_loss: 0.7297 - val_accuracy: 0.7529
Epoch 15/25
102/102 - 0s - loss: 0.6992 - accuracy: 0.7666 - val_loss: 0.7192 - val_accuracy: 0.7569
Epoch 16/25
102/102 - 0s - loss: 0.6884 - accuracy: 0.7700 - val_loss: 0.7108 - val_accuracy: 0.7587
Epoch 17/25
102/102 - 0s - loss: 0.6765 - accuracy: 0.7768 - val_loss: 0.7008 - val_accuracy: 0.7604
Epoch 18/25
102/102 - 0s - loss: 0.6663 - accuracy: 0.7804 - val_loss: 0.6915 - val_accuracy: 0.7691
Epoch 19/25
102/102 - 0s - loss: 0.6554 - accuracy: 0.7858 - val_loss: 0.6825 - val_accuracy: 0.7749
Epoch 20/25
102/102 - 0s - loss: 0.6477 - accuracy: 0.7841 - val_loss: 0.6727 - val_accuracy: 0.7714
Epoch 21/25
102/102 - 0s - loss: 0.6388 - accuracy: 0.7904 - val_loss: 0.6690 - val_accuracy: 0.7795
Epoch 22/25
102/102 - 0s - loss: 0.6299 - accuracy: 0.7918 - val_loss: 0.6615 - val_accuracy: 0.7836
Epoch 23/25
102/102 - 0s - loss: 0.6209 - accuracy: 0.7950 - val_loss: 0.6523 - val_accuracy: 0.7824
Epoch 24/25
102/102 - 0s - loss: 0.6130 - accuracy: 0.7990 - val_loss: 0.6436 - val_accuracy: 0.7876
Epoch 25/25
102/102 - 0s - loss: 0.6052 - accuracy: 0.7997 - val_loss: 0.6383 - val_accuracy: 0.7917
6/6 - 0s - loss: 0.8148 - accuracy: 0.6686


Loss and metrics for TANH model with 64 batches and 25 epochs: 
Test Loss is 0.81 
Test Accuracy is 66.86 %

Epoch 1/40
102/102 - 1s - loss: 1.2554 - accuracy: 0.4591 - val_loss: 1.1336 - val_accuracy: 0.5891
Epoch 2/40
102/102 - 0s - loss: 1.0771 - accuracy: 0.6101 - val_loss: 1.0233 - val_accuracy: 0.6464
Epoch 3/40
102/102 - 0s - loss: 0.9890 - accuracy: 0.6483 - val_loss: 0.9556 - val_accuracy: 0.6840
Epoch 4/40
102/102 - 0s - loss: 0.9320 - accuracy: 0.6800 - val_loss: 0.9146 - val_accuracy: 0.7060
Epoch 5/40
102/102 - 0s - loss: 0.8911 - accuracy: 0.6946 - val_loss: 0.8814 - val_accuracy: 0.7031
Epoch 6/40
102/102 - 0s - loss: 0.8585 - accuracy: 0.7070 - val_loss: 0.8573 - val_accuracy: 0.7054
Epoch 7/40
102/102 - 0s - loss: 0.8316 - accuracy: 0.7183 - val_loss: 0.8325 - val_accuracy: 0.7211
Epoch 8/40
102/102 - 0s - loss: 0.8090 - accuracy: 0.7278 - val_loss: 0.8129 - val_accuracy: 0.7211
Epoch 9/40
102/102 - 0s - loss: 0.7881 - accuracy: 0.7376 - val_loss: 0.7975 - val_accuracy: 0.7297
Epoch 10/40
102/102 - 0s - loss: 0.7694 - accuracy: 0.7437 - val_loss: 0.7806 - val_accuracy: 0.7344
Epoch 11/40
102/102 - 0s - loss: 0.7531 - accuracy: 0.7465 - val_loss: 0.7654 - val_accuracy: 0.7413
Epoch 12/40
102/102 - 0s - loss: 0.7367 - accuracy: 0.7536 - val_loss: 0.7552 - val_accuracy: 0.7459
Epoch 13/40
102/102 - 0s - loss: 0.7235 - accuracy: 0.7570 - val_loss: 0.7396 - val_accuracy: 0.7477
Epoch 14/40
102/102 - 0s - loss: 0.7094 - accuracy: 0.7675 - val_loss: 0.7272 - val_accuracy: 0.7523
Epoch 15/40
102/102 - 0s - loss: 0.6979 - accuracy: 0.7657 - val_loss: 0.7188 - val_accuracy: 0.7552
Epoch 16/40
102/102 - 0s - loss: 0.6866 - accuracy: 0.7715 - val_loss: 0.7083 - val_accuracy: 0.7650
Epoch 17/40
102/102 - 0s - loss: 0.6744 - accuracy: 0.7780 - val_loss: 0.7001 - val_accuracy: 0.7639
Epoch 18/40
102/102 - 0s - loss: 0.6640 - accuracy: 0.7825 - val_loss: 0.6892 - val_accuracy: 0.7726
Epoch 19/40
102/102 - 0s - loss: 0.6551 - accuracy: 0.7856 - val_loss: 0.6794 - val_accuracy: 0.7720
Epoch 20/40
102/102 - 0s - loss: 0.6451 - accuracy: 0.7876 - val_loss: 0.6712 - val_accuracy: 0.7772
Epoch 21/40
102/102 - 0s - loss: 0.6354 - accuracy: 0.7915 - val_loss: 0.6638 - val_accuracy: 0.7778
Epoch 22/40
102/102 - 0s - loss: 0.6279 - accuracy: 0.7919 - val_loss: 0.6557 - val_accuracy: 0.7807
Epoch 23/40
102/102 - 0s - loss: 0.6189 - accuracy: 0.7949 - val_loss: 0.6574 - val_accuracy: 0.7812
Epoch 24/40
102/102 - 0s - loss: 0.6121 - accuracy: 0.7998 - val_loss: 0.6414 - val_accuracy: 0.7882
Epoch 25/40
102/102 - 0s - loss: 0.6031 - accuracy: 0.8014 - val_loss: 0.6338 - val_accuracy: 0.7917
Epoch 26/40
102/102 - 0s - loss: 0.5959 - accuracy: 0.8009 - val_loss: 0.6290 - val_accuracy: 0.7963
Epoch 27/40
102/102 - 0s - loss: 0.5882 - accuracy: 0.8052 - val_loss: 0.6214 - val_accuracy: 0.7969
Epoch 28/40
102/102 - 0s - loss: 0.5816 - accuracy: 0.8088 - val_loss: 0.6157 - val_accuracy: 0.7963
Epoch 29/40
102/102 - 0s - loss: 0.5745 - accuracy: 0.8095 - val_loss: 0.6107 - val_accuracy: 0.7992
Epoch 30/40
102/102 - 0s - loss: 0.5681 - accuracy: 0.8094 - val_loss: 0.6053 - val_accuracy: 0.7975
Epoch 31/40
102/102 - 0s - loss: 0.5617 - accuracy: 0.8140 - val_loss: 0.5961 - val_accuracy: 0.8079
Epoch 32/40
102/102 - 0s - loss: 0.5557 - accuracy: 0.8162 - val_loss: 0.5918 - val_accuracy: 0.8090
Epoch 33/40
102/102 - 0s - loss: 0.5490 - accuracy: 0.8211 - val_loss: 0.5869 - val_accuracy: 0.8084
Epoch 34/40
102/102 - 0s - loss: 0.5430 - accuracy: 0.8208 - val_loss: 0.5851 - val_accuracy: 0.8096
Epoch 35/40
102/102 - 0s - loss: 0.5389 - accuracy: 0.8242 - val_loss: 0.5791 - val_accuracy: 0.8119
Epoch 36/40
102/102 - 0s - loss: 0.5324 - accuracy: 0.8255 - val_loss: 0.5764 - val_accuracy: 0.8142
Epoch 37/40
102/102 - 0s - loss: 0.5266 - accuracy: 0.8284 - val_loss: 0.5669 - val_accuracy: 0.8142
Epoch 38/40
102/102 - 0s - loss: 0.5213 - accuracy: 0.8299 - val_loss: 0.5612 - val_accuracy: 0.8160
Epoch 39/40
102/102 - 0s - loss: 0.5167 - accuracy: 0.8302 - val_loss: 0.5600 - val_accuracy: 0.8189
Epoch 40/40
102/102 - 0s - loss: 0.5112 - accuracy: 0.8326 - val_loss: 0.5557 - val_accuracy: 0.8160
6/6 - 0s - loss: 0.8110 - accuracy: 0.6457


Loss and metrics for TANH model with 64 batches and 40 epochs: 
Test Loss is 0.81 
Test Accuracy is 64.57 %

Epoch 1/50
203/203 - 1s - loss: 1.2695 - accuracy: 0.4424 - val_loss: 1.1262 - val_accuracy: 0.6262
Epoch 2/50
203/203 - 1s - loss: 1.0531 - accuracy: 0.6576 - val_loss: 0.9866 - val_accuracy: 0.7037
Epoch 3/50
203/203 - 1s - loss: 0.9458 - accuracy: 0.7093 - val_loss: 0.9046 - val_accuracy: 0.7355
Epoch 4/50
203/203 - 1s - loss: 0.8738 - accuracy: 0.7482 - val_loss: 0.8449 - val_accuracy: 0.7500
Epoch 5/50
203/203 - 1s - loss: 0.8186 - accuracy: 0.7649 - val_loss: 0.7999 - val_accuracy: 0.7622
Epoch 6/50
203/203 - 1s - loss: 0.7749 - accuracy: 0.7762 - val_loss: 0.7629 - val_accuracy: 0.7714
Epoch 7/50
203/203 - 1s - loss: 0.7360 - accuracy: 0.7881 - val_loss: 0.7316 - val_accuracy: 0.7801
Epoch 8/50
203/203 - 1s - loss: 0.7025 - accuracy: 0.7977 - val_loss: 0.7008 - val_accuracy: 0.7824
Epoch 9/50
203/203 - 1s - loss: 0.6730 - accuracy: 0.7987 - val_loss: 0.6753 - val_accuracy: 0.7957
Epoch 10/50
203/203 - 1s - loss: 0.6473 - accuracy: 0.8066 - val_loss: 0.6505 - val_accuracy: 0.7980
Epoch 11/50
203/203 - 1s - loss: 0.6245 - accuracy: 0.8128 - val_loss: 0.6325 - val_accuracy: 0.7922
Epoch 12/50
203/203 - 1s - loss: 0.6024 - accuracy: 0.8165 - val_loss: 0.6133 - val_accuracy: 0.7963
Epoch 13/50
203/203 - 1s - loss: 0.5844 - accuracy: 0.8234 - val_loss: 0.5939 - val_accuracy: 0.8090
Epoch 14/50
203/203 - 1s - loss: 0.5666 - accuracy: 0.8282 - val_loss: 0.5784 - val_accuracy: 0.8171
Epoch 15/50
203/203 - 1s - loss: 0.5502 - accuracy: 0.8332 - val_loss: 0.5620 - val_accuracy: 0.8206
Epoch 16/50
203/203 - 1s - loss: 0.5353 - accuracy: 0.8352 - val_loss: 0.5497 - val_accuracy: 0.8241
Epoch 17/50
203/203 - 1s - loss: 0.5225 - accuracy: 0.8392 - val_loss: 0.5388 - val_accuracy: 0.8264
Epoch 18/50
203/203 - 1s - loss: 0.5103 - accuracy: 0.8421 - val_loss: 0.5259 - val_accuracy: 0.8304
Epoch 19/50
203/203 - 1s - loss: 0.4971 - accuracy: 0.8460 - val_loss: 0.5185 - val_accuracy: 0.8351
Epoch 20/50
203/203 - 1s - loss: 0.4875 - accuracy: 0.8496 - val_loss: 0.5035 - val_accuracy: 0.8397
Epoch 21/50
203/203 - 1s - loss: 0.4774 - accuracy: 0.8548 - val_loss: 0.4949 - val_accuracy: 0.8461
Epoch 22/50
203/203 - 1s - loss: 0.4669 - accuracy: 0.8556 - val_loss: 0.4883 - val_accuracy: 0.8478
Epoch 23/50
203/203 - 1s - loss: 0.4587 - accuracy: 0.8584 - val_loss: 0.4790 - val_accuracy: 0.8524
Epoch 24/50
203/203 - 1s - loss: 0.4504 - accuracy: 0.8633 - val_loss: 0.4718 - val_accuracy: 0.8513
Epoch 25/50
203/203 - 1s - loss: 0.4424 - accuracy: 0.8635 - val_loss: 0.4643 - val_accuracy: 0.8530
Epoch 26/50
203/203 - 1s - loss: 0.4352 - accuracy: 0.8633 - val_loss: 0.4574 - val_accuracy: 0.8553
Epoch 27/50
203/203 - 1s - loss: 0.4280 - accuracy: 0.8687 - val_loss: 0.4508 - val_accuracy: 0.8600
Epoch 28/50
203/203 - 1s - loss: 0.4207 - accuracy: 0.8678 - val_loss: 0.4449 - val_accuracy: 0.8663
Epoch 29/50
203/203 - 1s - loss: 0.4146 - accuracy: 0.8710 - val_loss: 0.4382 - val_accuracy: 0.8657
Epoch 30/50
203/203 - 1s - loss: 0.4087 - accuracy: 0.8704 - val_loss: 0.4347 - val_accuracy: 0.8675
Epoch 31/50
203/203 - 1s - loss: 0.4027 - accuracy: 0.8749 - val_loss: 0.4315 - val_accuracy: 0.8692
Epoch 32/50
203/203 - 1s - loss: 0.3977 - accuracy: 0.8747 - val_loss: 0.4262 - val_accuracy: 0.8692
Epoch 33/50
203/203 - 1s - loss: 0.3920 - accuracy: 0.8755 - val_loss: 0.4187 - val_accuracy: 0.8698
Epoch 34/50
203/203 - 1s - loss: 0.3866 - accuracy: 0.8766 - val_loss: 0.4126 - val_accuracy: 0.8756
Epoch 35/50
203/203 - 1s - loss: 0.3813 - accuracy: 0.8797 - val_loss: 0.4096 - val_accuracy: 0.8796
Epoch 36/50
203/203 - 1s - loss: 0.3773 - accuracy: 0.8803 - val_loss: 0.4060 - val_accuracy: 0.8773
Epoch 37/50
203/203 - 1s - loss: 0.3724 - accuracy: 0.8811 - val_loss: 0.4012 - val_accuracy: 0.8808
Epoch 38/50
203/203 - 1s - loss: 0.3679 - accuracy: 0.8811 - val_loss: 0.3998 - val_accuracy: 0.8814
Epoch 39/50
203/203 - 1s - loss: 0.3637 - accuracy: 0.8821 - val_loss: 0.3922 - val_accuracy: 0.8872
Epoch 40/50
203/203 - 1s - loss: 0.3594 - accuracy: 0.8845 - val_loss: 0.3916 - val_accuracy: 0.8825
Epoch 41/50
203/203 - 1s - loss: 0.3562 - accuracy: 0.8846 - val_loss: 0.3850 - val_accuracy: 0.8872
Epoch 42/50
203/203 - 1s - loss: 0.3521 - accuracy: 0.8859 - val_loss: 0.3806 - val_accuracy: 0.8854
Epoch 43/50
203/203 - 1s - loss: 0.3482 - accuracy: 0.8891 - val_loss: 0.3802 - val_accuracy: 0.8854
Epoch 44/50
203/203 - 1s - loss: 0.3451 - accuracy: 0.8897 - val_loss: 0.3744 - val_accuracy: 0.8872
Epoch 45/50
203/203 - 1s - loss: 0.3413 - accuracy: 0.8900 - val_loss: 0.3706 - val_accuracy: 0.8918
Epoch 46/50
203/203 - 1s - loss: 0.3369 - accuracy: 0.8922 - val_loss: 0.3711 - val_accuracy: 0.8866
Epoch 47/50
203/203 - 1s - loss: 0.3336 - accuracy: 0.8956 - val_loss: 0.3670 - val_accuracy: 0.8895
Epoch 48/50
203/203 - 1s - loss: 0.3295 - accuracy: 0.8951 - val_loss: 0.3635 - val_accuracy: 0.8889
Epoch 49/50
203/203 - 1s - loss: 0.3267 - accuracy: 0.8957 - val_loss: 0.3635 - val_accuracy: 0.8929
Epoch 50/50
203/203 - 1s - loss: 0.3247 - accuracy: 0.8951 - val_loss: 0.3634 - val_accuracy: 0.8929
6/6 - 0s - loss: 0.8810 - accuracy: 0.6686


Loss and metrics for RELU model with 32 batches and 50 epochs: 
Test Loss is 0.88 
Test Accuracy is 66.86 %

Epoch 1/50
203/203 - 2s - loss: 1.0172 - accuracy: 0.5938 - val_loss: 0.9343 - val_accuracy: 0.6493
Epoch 2/50
203/203 - 1s - loss: 0.7313 - accuracy: 0.7302 - val_loss: 0.6868 - val_accuracy: 0.7627
Epoch 3/50
203/203 - 1s - loss: 0.6156 - accuracy: 0.7833 - val_loss: 0.5886 - val_accuracy: 0.7998
Epoch 4/50
203/203 - 1s - loss: 0.5548 - accuracy: 0.8086 - val_loss: 0.5326 - val_accuracy: 0.8206
Epoch 5/50
203/203 - 1s - loss: 0.5117 - accuracy: 0.8261 - val_loss: 0.4862 - val_accuracy: 0.8380
Epoch 6/50
203/203 - 1s - loss: 0.4847 - accuracy: 0.8383 - val_loss: 0.4596 - val_accuracy: 0.8507
Epoch 7/50
203/203 - 1s - loss: 0.4628 - accuracy: 0.8480 - val_loss: 0.4638 - val_accuracy: 0.8519
Epoch 8/50
203/203 - 1s - loss: 0.4339 - accuracy: 0.8587 - val_loss: 0.4270 - val_accuracy: 0.8698
Epoch 9/50
203/203 - 1s - loss: 0.4215 - accuracy: 0.8605 - val_loss: 0.4296 - val_accuracy: 0.8669
Epoch 10/50
203/203 - 1s - loss: 0.4104 - accuracy: 0.8658 - val_loss: 0.4105 - val_accuracy: 0.8733
Epoch 11/50
203/203 - 1s - loss: 0.3972 - accuracy: 0.8696 - val_loss: 0.4033 - val_accuracy: 0.8785
Epoch 12/50
203/203 - 1s - loss: 0.3862 - accuracy: 0.8736 - val_loss: 0.4118 - val_accuracy: 0.8796
Epoch 13/50
203/203 - 1s - loss: 0.3773 - accuracy: 0.8775 - val_loss: 0.3809 - val_accuracy: 0.8819
Epoch 14/50
203/203 - 1s - loss: 0.3629 - accuracy: 0.8848 - val_loss: 0.3652 - val_accuracy: 0.8912
Epoch 15/50
203/203 - 1s - loss: 0.3578 - accuracy: 0.8846 - val_loss: 0.3867 - val_accuracy: 0.8819
Epoch 16/50
203/203 - 1s - loss: 0.3521 - accuracy: 0.8865 - val_loss: 0.3781 - val_accuracy: 0.8854
Epoch 17/50
203/203 - 1s - loss: 0.3416 - accuracy: 0.8909 - val_loss: 0.3644 - val_accuracy: 0.8912
Epoch 18/50
203/203 - 1s - loss: 0.3337 - accuracy: 0.8950 - val_loss: 0.3515 - val_accuracy: 0.8929
Epoch 19/50
203/203 - 1s - loss: 0.3317 - accuracy: 0.8926 - val_loss: 0.3581 - val_accuracy: 0.8866
Epoch 20/50
203/203 - 1s - loss: 0.3234 - accuracy: 0.8960 - val_loss: 0.3543 - val_accuracy: 0.8889
Epoch 21/50
203/203 - 1s - loss: 0.3149 - accuracy: 0.9008 - val_loss: 0.3613 - val_accuracy: 0.8906
Epoch 22/50
203/203 - 1s - loss: 0.3051 - accuracy: 0.8999 - val_loss: 0.3464 - val_accuracy: 0.8918
Epoch 23/50
203/203 - 1s - loss: 0.3042 - accuracy: 0.9035 - val_loss: 0.3385 - val_accuracy: 0.8964
Epoch 24/50
203/203 - 1s - loss: 0.3032 - accuracy: 0.9019 - val_loss: 0.3555 - val_accuracy: 0.8964
Epoch 25/50
203/203 - 1s - loss: 0.2945 - accuracy: 0.9058 - val_loss: 0.3165 - val_accuracy: 0.9068
Epoch 26/50
203/203 - 1s - loss: 0.2938 - accuracy: 0.9067 - val_loss: 0.3354 - val_accuracy: 0.8993
Epoch 27/50
203/203 - 1s - loss: 0.2837 - accuracy: 0.9081 - val_loss: 0.3296 - val_accuracy: 0.8964
Epoch 28/50
203/203 - 1s - loss: 0.2867 - accuracy: 0.9103 - val_loss: 0.3184 - val_accuracy: 0.9022
Epoch 29/50
203/203 - 1s - loss: 0.2833 - accuracy: 0.9090 - val_loss: 0.3283 - val_accuracy: 0.8987
Epoch 30/50
203/203 - 1s - loss: 0.2749 - accuracy: 0.9135 - val_loss: 0.3385 - val_accuracy: 0.8941
Epoch 31/50
203/203 - 1s - loss: 0.2733 - accuracy: 0.9171 - val_loss: 0.3076 - val_accuracy: 0.9068
Epoch 32/50
203/203 - 1s - loss: 0.2694 - accuracy: 0.9167 - val_loss: 0.2990 - val_accuracy: 0.9062
Epoch 33/50
203/203 - 1s - loss: 0.2618 - accuracy: 0.9206 - val_loss: 0.3069 - val_accuracy: 0.9068
Epoch 34/50
203/203 - 1s - loss: 0.2645 - accuracy: 0.9164 - val_loss: 0.3079 - val_accuracy: 0.9045
Epoch 35/50
203/203 - 1s - loss: 0.2597 - accuracy: 0.9203 - val_loss: 0.3422 - val_accuracy: 0.8918
Epoch 36/50
203/203 - 1s - loss: 0.2544 - accuracy: 0.9215 - val_loss: 0.3322 - val_accuracy: 0.8924
Epoch 37/50
203/203 - 1s - loss: 0.2535 - accuracy: 0.9192 - val_loss: 0.3142 - val_accuracy: 0.8999
Epoch 38/50
203/203 - 1s - loss: 0.2481 - accuracy: 0.9276 - val_loss: 0.3041 - val_accuracy: 0.9068
Epoch 39/50
203/203 - 1s - loss: 0.2491 - accuracy: 0.9263 - val_loss: 0.2883 - val_accuracy: 0.9138
Epoch 40/50
203/203 - 1s - loss: 0.2433 - accuracy: 0.9248 - val_loss: 0.2910 - val_accuracy: 0.9120
Epoch 41/50
203/203 - 1s - loss: 0.2414 - accuracy: 0.9245 - val_loss: 0.3037 - val_accuracy: 0.9034
Epoch 42/50
203/203 - 1s - loss: 0.2409 - accuracy: 0.9228 - val_loss: 0.3073 - val_accuracy: 0.9045
Epoch 43/50
203/203 - 1s - loss: 0.2362 - accuracy: 0.9274 - val_loss: 0.3020 - val_accuracy: 0.9051
Epoch 44/50
203/203 - 1s - loss: 0.2384 - accuracy: 0.9263 - val_loss: 0.2941 - val_accuracy: 0.9045
Epoch 45/50
203/203 - 1s - loss: 0.2335 - accuracy: 0.9252 - val_loss: 0.2976 - val_accuracy: 0.9074
Epoch 46/50
203/203 - 1s - loss: 0.2323 - accuracy: 0.9300 - val_loss: 0.2886 - val_accuracy: 0.9091
Epoch 47/50
203/203 - 1s - loss: 0.2254 - accuracy: 0.9325 - val_loss: 0.2812 - val_accuracy: 0.9080
Epoch 48/50
203/203 - 1s - loss: 0.2229 - accuracy: 0.9328 - val_loss: 0.2826 - val_accuracy: 0.9138
Epoch 49/50
203/203 - 1s - loss: 0.2234 - accuracy: 0.9327 - val_loss: 0.2806 - val_accuracy: 0.9155
Epoch 50/50
203/203 - 1s - loss: 0.2215 - accuracy: 0.9305 - val_loss: 0.2767 - val_accuracy: 0.9172
6/6 - 0s - loss: 0.8453 - accuracy: 0.6629


Loss and metrics for TANH model with batch normalization, 32 batches and 50 epochs: 
Test Loss is 0.85 
Test Accuracy is 66.29 %

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
permute (Permute)            (None, 32, 32, 1)         0         
_________________________________________________________________
Conv2D_1 (Conv2D)            (None, 32, 32, 64)        640       
_________________________________________________________________
dropout (Dropout)            (None, 32, 32, 64)        0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 32, 32, 64)        128       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         
_________________________________________________________________
Conv2D_2 (Conv2D)            (None, 16, 16, 128)       73856     
_________________________________________________________________
dropout_1 (Dropout)          (None, 16, 16, 128)       0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 16, 16, 128)       64        
_________________________________________________________________
Conv2D_3 (Conv2D)            (None, 16, 16, 128)       147584    
_________________________________________________________________
dropout_2 (Dropout)          (None, 16, 16, 128)       0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 16, 16, 128)       64        
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         
_________________________________________________________________
Conv2D_4 (Conv2D)            (None, 8, 8, 256)         295168    
_________________________________________________________________
dropout_3 (Dropout)          (None, 8, 8, 256)         0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 8, 8, 256)         32        
_________________________________________________________________
Conv2D_5 (Conv2D)            (None, 8, 8, 256)         590080    
_________________________________________________________________
dropout_4 (Dropout)          (None, 8, 8, 256)         0         
_________________________________________________________________
batch_normalization_6 (Batch (None, 8, 8, 256)         32        
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4096)              0         
_________________________________________________________________
FCN_1 (Dense)                (None, 512)               2097664   
_________________________________________________________________
dropout_5 (Dropout)          (None, 512)               0         
_________________________________________________________________
FCN_2 (Dense)                (None, 128)               65664     
_________________________________________________________________
FCN_3 (Dense)                (None, 4)                 516       
=================================================================
Total params: 3,271,492
Trainable params: 3,271,332
Non-trainable params: 160
_________________________________________________________________
Epoch 1/25
102/102 [==============================] - 20s 184ms/step - loss: 8.3347 - acc: 0.3878 - val_loss: 8.0648 - val_acc: 0.2500
Epoch 2/25
102/102 [==============================] - 17s 170ms/step - loss: 7.7033 - acc: 0.5261 - val_loss: 8.4281 - val_acc: 0.2500
Epoch 3/25
102/102 [==============================] - 17s 166ms/step - loss: 7.5151 - acc: 0.5621 - val_loss: 8.6840 - val_acc: 0.2500
Epoch 4/25
102/102 [==============================] - 17s 165ms/step - loss: 7.3920 - acc: 0.5975 - val_loss: 8.7226 - val_acc: 0.2500
Epoch 5/25
102/102 [==============================] - 16s 160ms/step - loss: 7.2741 - acc: 0.6343 - val_loss: 8.6369 - val_acc: 0.2494
Epoch 6/25
102/102 [==============================] - 16s 161ms/step - loss: 7.2123 - acc: 0.6465 - val_loss: 8.3241 - val_acc: 0.2535
Epoch 7/25
102/102 [==============================] - 16s 158ms/step - loss: 7.1554 - acc: 0.6654 - val_loss: 8.1252 - val_acc: 0.3148
Epoch 8/25
102/102 [==============================] - 16s 161ms/step - loss: 7.0844 - acc: 0.6863 - val_loss: 8.0632 - val_acc: 0.3362
Epoch 9/25
102/102 [==============================] - 16s 161ms/step - loss: 7.0222 - acc: 0.6941 - val_loss: 8.0887 - val_acc: 0.3380
Epoch 10/25
102/102 [==============================] - 16s 159ms/step - loss: 6.9740 - acc: 0.7181 - val_loss: 8.0500 - val_acc: 0.3449
Epoch 11/25
102/102 [==============================] - 16s 162ms/step - loss: 6.9509 - acc: 0.7059 - val_loss: 7.9662 - val_acc: 0.3675
Epoch 12/25
102/102 [==============================] - 17s 162ms/step - loss: 6.9078 - acc: 0.7215 - val_loss: 7.9636 - val_acc: 0.3617
Epoch 13/25
102/102 [==============================] - 17s 167ms/step - loss: 6.8645 - acc: 0.7357 - val_loss: 7.9770 - val_acc: 0.3652
Epoch 14/25
102/102 [==============================] - 16s 160ms/step - loss: 6.8192 - acc: 0.7412 - val_loss: 7.9676 - val_acc: 0.3704
Epoch 15/25
102/102 [==============================] - 17s 162ms/step - loss: 6.7748 - acc: 0.7550 - val_loss: 7.9948 - val_acc: 0.3675
Epoch 16/25
102/102 [==============================] - 16s 161ms/step - loss: 6.7606 - acc: 0.7668 - val_loss: 7.9410 - val_acc: 0.3808
Epoch 17/25
102/102 [==============================] - 16s 161ms/step - loss: 6.7264 - acc: 0.7593 - val_loss: 7.9225 - val_acc: 0.3785
Epoch 18/25
102/102 [==============================] - 16s 159ms/step - loss: 6.6965 - acc: 0.7777 - val_loss: 7.9355 - val_acc: 0.3791
Epoch 19/25
102/102 [==============================] - 16s 159ms/step - loss: 6.6594 - acc: 0.7763 - val_loss: 7.9285 - val_acc: 0.3773
Epoch 20/25
102/102 [==============================] - 16s 160ms/step - loss: 6.6396 - acc: 0.7748 - val_loss: 7.8805 - val_acc: 0.3900
Epoch 21/25
102/102 [==============================] - 16s 157ms/step - loss: 6.5968 - acc: 0.7911 - val_loss: 7.8827 - val_acc: 0.3854
Epoch 22/25
102/102 [==============================] - 16s 157ms/step - loss: 6.5844 - acc: 0.7977 - val_loss: 7.8930 - val_acc: 0.3883
Epoch 23/25
102/102 [==============================] - 16s 159ms/step - loss: 6.5604 - acc: 0.7930 - val_loss: 7.8752 - val_acc: 0.3906
Epoch 24/25
102/102 [==============================] - 16s 160ms/step - loss: 6.5590 - acc: 0.7858 - val_loss: 7.8242 - val_acc: 0.4010
Epoch 25/25
102/102 [==============================] - 16s 160ms/step - loss: 6.5010 - acc: 0.8104 - val_loss: 7.8343 - val_acc: 0.4028
6/6 [==============================] - 0s 18ms/step - loss: 8.6052 - acc: 0.2629
test loss, test acc: [8.60518741607666, 0.2628571391105652]
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
permute_1 (Permute)          (None, 32, 32, 1)         0         
_________________________________________________________________
Conv2D_1 (Conv2D)            (None, 32, 32, 32)        320       
_________________________________________________________________
dropout_6 (Dropout)          (None, 32, 32, 32)        0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 32, 32, 32)        128       
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 16, 16, 32)        0         
_________________________________________________________________
Conv2D_2 (Conv2D)            (None, 16, 16, 64)        18496     
_________________________________________________________________
dropout_7 (Dropout)          (None, 16, 16, 64)        0         
_________________________________________________________________
batch_normalization_8 (Batch (None, 16, 16, 64)        64        
_________________________________________________________________
Conv2D_3 (Conv2D)            (None, 16, 16, 64)        36928     
_________________________________________________________________
dropout_8 (Dropout)          (None, 16, 16, 64)        0         
_________________________________________________________________
batch_normalization_9 (Batch (None, 16, 16, 64)        64        
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 8, 8, 64)          0         
_________________________________________________________________
Conv2D_4 (Conv2D)            (None, 8, 8, 128)         73856     
_________________________________________________________________
dropout_9 (Dropout)          (None, 8, 8, 128)         0         
_________________________________________________________________
batch_normalization_10 (Batc (None, 8, 8, 128)         32        
_________________________________________________________________
Conv2D_5 (Conv2D)            (None, 8, 8, 128)         147584    
_________________________________________________________________
dropout_10 (Dropout)         (None, 8, 8, 128)         0         
_________________________________________________________________
batch_normalization_11 (Batc (None, 8, 8, 128)         32        
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 4, 4, 128)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 2048)              0         
_________________________________________________________________
FCN_1 (Dense)                (None, 512)               1049088   
_________________________________________________________________
dropout_11 (Dropout)         (None, 512)               0         
_________________________________________________________________
FCN_2 (Dense)                (None, 128)               65664     
_________________________________________________________________
FCN_3 (Dense)                (None, 4)                 516       
=================================================================
Total params: 1,392,772
Trainable params: 1,392,612
Non-trainable params: 160
_________________________________________________________________
Epoch 1/25
102/102 [==============================] - 9s 80ms/step - loss: 5.1974 - acc: 0.3425 - val_loss: 4.8296 - val_acc: 0.2500
Epoch 2/25
102/102 [==============================] - 8s 77ms/step - loss: 4.6008 - acc: 0.4998 - val_loss: 5.0020 - val_acc: 0.2500
Epoch 3/25
102/102 [==============================] - 8s 78ms/step - loss: 4.4591 - acc: 0.5206 - val_loss: 5.1364 - val_acc: 0.2500
Epoch 4/25
102/102 [==============================] - 8s 76ms/step - loss: 4.4123 - acc: 0.5383 - val_loss: 5.1199 - val_acc: 0.2500
Epoch 5/25
102/102 [==============================] - 8s 76ms/step - loss: 4.3725 - acc: 0.5508 - val_loss: 5.0867 - val_acc: 0.2500
Epoch 6/25
102/102 [==============================] - 8s 76ms/step - loss: 4.2942 - acc: 0.5774 - val_loss: 5.0185 - val_acc: 0.2465
Epoch 7/25
102/102 [==============================] - 8s 76ms/step - loss: 4.2631 - acc: 0.5873 - val_loss: 4.9974 - val_acc: 0.2471
Epoch 8/25
102/102 [==============================] - 8s 76ms/step - loss: 4.2385 - acc: 0.5965 - val_loss: 4.9863 - val_acc: 0.2477
Epoch 9/25
102/102 [==============================] - 8s 75ms/step - loss: 4.2083 - acc: 0.6012 - val_loss: 4.9747 - val_acc: 0.2506
Epoch 10/25
102/102 [==============================] - 8s 75ms/step - loss: 4.2329 - acc: 0.5875 - val_loss: 4.9733 - val_acc: 0.2517
Epoch 11/25
102/102 [==============================] - 8s 77ms/step - loss: 4.2028 - acc: 0.6015 - val_loss: 4.9607 - val_acc: 0.2529
Epoch 12/25
102/102 [==============================] - 8s 78ms/step - loss: 4.1608 - acc: 0.6157 - val_loss: 4.9508 - val_acc: 0.2552
Epoch 13/25
102/102 [==============================] - 8s 77ms/step - loss: 4.1289 - acc: 0.6252 - val_loss: 4.9663 - val_acc: 0.2529
Epoch 14/25
102/102 [==============================] - 8s 76ms/step - loss: 4.1185 - acc: 0.6289 - val_loss: 4.9676 - val_acc: 0.2529
Epoch 15/25
102/102 [==============================] - 8s 77ms/step - loss: 4.1240 - acc: 0.6259 - val_loss: 4.9459 - val_acc: 0.2541
Epoch 16/25
102/102 [==============================] - 8s 77ms/step - loss: 4.0739 - acc: 0.6365 - val_loss: 4.9532 - val_acc: 0.2541
Epoch 17/25
102/102 [==============================] - 8s 77ms/step - loss: 4.0983 - acc: 0.6307 - val_loss: 4.9781 - val_acc: 0.2529
Epoch 18/25
102/102 [==============================] - 8s 77ms/step - loss: 4.0707 - acc: 0.6359 - val_loss: 4.9590 - val_acc: 0.2535
Epoch 19/25
102/102 [==============================] - 8s 76ms/step - loss: 4.0710 - acc: 0.6293 - val_loss: 4.9597 - val_acc: 0.2535
Epoch 20/25
102/102 [==============================] - 8s 76ms/step - loss: 4.0250 - acc: 0.6451 - val_loss: 4.9723 - val_acc: 0.2529
Epoch 21/25
102/102 [==============================] - 8s 77ms/step - loss: 4.0407 - acc: 0.6491 - val_loss: 4.9674 - val_acc: 0.2541
Epoch 22/25
102/102 [==============================] - 8s 74ms/step - loss: 4.0177 - acc: 0.6537 - val_loss: 4.9868 - val_acc: 0.2529
Epoch 23/25
102/102 [==============================] - 8s 74ms/step - loss: 4.0224 - acc: 0.6607 - val_loss: 4.9741 - val_acc: 0.2529
Epoch 24/25
102/102 [==============================] - 8s 75ms/step - loss: 4.0321 - acc: 0.6436 - val_loss: 4.9786 - val_acc: 0.2535
Epoch 25/25
102/102 [==============================] - 8s 76ms/step - loss: 3.9933 - acc: 0.6591 - val_loss: 4.9741 - val_acc: 0.2552
6/6 [==============================] - 0s 11ms/step - loss: 5.4063 - acc: 0.2514
test loss, test acc: [5.406332492828369, 0.2514285743236542]

All the results we got are:

                  RELU (64 batches, 25 epochs)  TANH (64 batches, 25 epochs)  \
Test loss                             0.779142                      0.814773   
Test accuracy(%)                     69.714284                     66.857141   

                  TANH (64 batches, 40 epochs)  RELU (32 batches, 50 epochs)  \
Test loss                             0.811045                      0.881014   
Test accuracy(%)                     64.571428                     66.857141   

                  TANH (batch normalized, 32 batches, 50 epochs)  \
Test loss                                               0.845286   
Test accuracy(%)                                       66.285712   

                  NNet ([64,128,128,256,256] filters)  \
Test loss                                    8.605187   
Test accuracy(%)                            26.285714   

                  NNet ([32,64,64,128,128] filters)  
Test loss                                  5.406332  
Test accuracy(%)                          25.142857  


(2ndPaper-t2) stu4@triton01:~/HW4_yoel_amit$ 
