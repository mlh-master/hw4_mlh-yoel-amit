(2ndPaper-t2) stu4@triton01:~/HW4_yoel_amit$ python HW4.py
2021-03-04 15:17:36.114659: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-03-04 15:17:36.114692: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-03-04 15:17:39.787516: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-04 15:17:39.794105: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-04 15:17:39.796277: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-04 15:17:39.817315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:5e:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 7.77GiB deviceMemoryBandwidth: 417.23GiB/s
2021-03-04 15:17:39.817401: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-03-04 15:17:39.817507: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2021-03-04 15:17:39.817579: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2021-03-04 15:17:39.819617: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-04 15:17:39.819944: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-04 15:17:39.821975: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-04 15:17:39.822076: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2021-03-04 15:17:39.822131: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2021-03-04 15:17:39.822141: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-03-04 15:17:39.918675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-04 15:17:39.918713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-03-04 15:17:39.918720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
WARNING:tensorflow:From HW4.py:52: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.

2021-03-04 15:18:09.490192: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-04 15:18:09.495241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:5e:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 7.77GiB deviceMemoryBandwidth: 417.23GiB/s
2021-03-04 15:18:09.495483: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-03-04 15:18:09.495560: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2021-03-04 15:18:09.495626: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2021-03-04 15:18:09.495651: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-04 15:18:09.495670: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-04 15:18:09.495688: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-04 15:18:09.495747: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2021-03-04 15:18:09.495810: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2021-03-04 15:18:09.495823: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-03-04 15:18:09.496159: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-04 15:18:09.496185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-04 15:18:09.496194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      
Model: "RELU_64_batches"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 1024)              0         
_________________________________________________________________
dense (Dense)                (None, 300)               307500    
_________________________________________________________________
Relu1 (Activation)           (None, 300)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 150)               45150     
_________________________________________________________________
Relu2 (Activation)           (None, 150)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 4)                 604       
_________________________________________________________________
activation (Activation)      (None, 4)                 0         
=================================================================
Total params: 353,254
Trainable params: 353,254
Non-trainable params: 0
_________________________________________________________________
2021-03-04 15:18:09.711627: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-04 15:18:09.733752: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz
Epoch 1/25
102/102 - 1s - loss: 1.2848 - accuracy: 0.4691 - val_loss: 1.1719 - val_accuracy: 0.5845
Epoch 2/25
102/102 - 0s - loss: 1.1266 - accuracy: 0.6050 - val_loss: 1.0641 - val_accuracy: 0.6123
Epoch 3/25
102/102 - 0s - loss: 1.0347 - accuracy: 0.6406 - val_loss: 0.9896 - val_accuracy: 0.6638
Epoch 4/25
102/102 - 0s - loss: 0.9690 - accuracy: 0.6784 - val_loss: 0.9352 - val_accuracy: 0.6933
Epoch 5/25
102/102 - 0s - loss: 0.9189 - accuracy: 0.7078 - val_loss: 0.8938 - val_accuracy: 0.7216
Epoch 6/25
102/102 - 0s - loss: 0.8776 - accuracy: 0.7309 - val_loss: 0.8592 - val_accuracy: 0.7396
Epoch 7/25
102/102 - 0s - loss: 0.8424 - accuracy: 0.7458 - val_loss: 0.8277 - val_accuracy: 0.7546
Epoch 8/25
102/102 - 0s - loss: 0.8110 - accuracy: 0.7590 - val_loss: 0.8021 - val_accuracy: 0.7541
Epoch 9/25
102/102 - 0s - loss: 0.7825 - accuracy: 0.7669 - val_loss: 0.7740 - val_accuracy: 0.7650
Epoch 10/25
102/102 - 0s - loss: 0.7568 - accuracy: 0.7760 - val_loss: 0.7522 - val_accuracy: 0.7685
Epoch 11/25
102/102 - 0s - loss: 0.7328 - accuracy: 0.7827 - val_loss: 0.7305 - val_accuracy: 0.7801
Epoch 12/25
102/102 - 0s - loss: 0.7109 - accuracy: 0.7882 - val_loss: 0.7113 - val_accuracy: 0.7772
Epoch 13/25
102/102 - 0s - loss: 0.6904 - accuracy: 0.7933 - val_loss: 0.6944 - val_accuracy: 0.7894
Epoch 14/25
102/102 - 0s - loss: 0.6712 - accuracy: 0.8031 - val_loss: 0.6745 - val_accuracy: 0.7992
Epoch 15/25
102/102 - 0s - loss: 0.6552 - accuracy: 0.8043 - val_loss: 0.6600 - val_accuracy: 0.7998
Epoch 16/25
102/102 - 0s - loss: 0.6390 - accuracy: 0.8091 - val_loss: 0.6466 - val_accuracy: 0.8021
Epoch 17/25
102/102 - 0s - loss: 0.6251 - accuracy: 0.8125 - val_loss: 0.6322 - val_accuracy: 0.8090
Epoch 18/25
102/102 - 0s - loss: 0.6106 - accuracy: 0.8171 - val_loss: 0.6230 - val_accuracy: 0.8119
Epoch 19/25
102/102 - 0s - loss: 0.5976 - accuracy: 0.8214 - val_loss: 0.6115 - val_accuracy: 0.8171
Epoch 20/25
102/102 - 0s - loss: 0.5862 - accuracy: 0.8231 - val_loss: 0.5981 - val_accuracy: 0.8137
Epoch 21/25
102/102 - 0s - loss: 0.5743 - accuracy: 0.8261 - val_loss: 0.5851 - val_accuracy: 0.8212
Epoch 22/25
102/102 - 0s - loss: 0.5635 - accuracy: 0.8292 - val_loss: 0.5789 - val_accuracy: 0.8235
Epoch 23/25
102/102 - 0s - loss: 0.5534 - accuracy: 0.8330 - val_loss: 0.5662 - val_accuracy: 0.8310
Epoch 24/25
102/102 - 0s - loss: 0.5436 - accuracy: 0.8335 - val_loss: 0.5560 - val_accuracy: 0.8351
Epoch 25/25
102/102 - 0s - loss: 0.5340 - accuracy: 0.8361 - val_loss: 0.5473 - val_accuracy: 0.8339
6/6 - 0s - loss: 0.8002 - accuracy: 0.6914


Loss and metrics for RELU model with 64 batches and 25 epochs: 
Test Loss is 0.80 
Test Accuracy is 69.14 %

Model: "TANH"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 300)               307500    
_________________________________________________________________
tanh1 (Activation)           (None, 300)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 150)               45150     
_________________________________________________________________
tanh2 (Activation)           (None, 150)               0         
_________________________________________________________________
dense_5 (Dense)              (None, 4)                 604       
_________________________________________________________________
activation_1 (Activation)    (None, 4)                 0         
=================================================================
Total params: 353,254
Trainable params: 353,254
Non-trainable params: 0
_________________________________________________________________
Epoch 1/25
102/102 - 1s - loss: 1.2566 - accuracy: 0.4611 - val_loss: 1.1473 - val_accuracy: 0.5411
Epoch 2/25
102/102 - 0s - loss: 1.0849 - accuracy: 0.5893 - val_loss: 1.0319 - val_accuracy: 0.6285
Epoch 3/25
102/102 - 0s - loss: 0.9933 - accuracy: 0.6455 - val_loss: 0.9666 - val_accuracy: 0.6713
Epoch 4/25
102/102 - 0s - loss: 0.9325 - accuracy: 0.6846 - val_loss: 0.9184 - val_accuracy: 0.6973
Epoch 5/25
102/102 - 0s - loss: 0.8883 - accuracy: 0.7040 - val_loss: 0.8808 - val_accuracy: 0.7066
Epoch 6/25
102/102 - 0s - loss: 0.8542 - accuracy: 0.7095 - val_loss: 0.8545 - val_accuracy: 0.7118
Epoch 7/25
102/102 - 0s - loss: 0.8276 - accuracy: 0.7167 - val_loss: 0.8312 - val_accuracy: 0.7297
Epoch 8/25
102/102 - 0s - loss: 0.8037 - accuracy: 0.7309 - val_loss: 0.8128 - val_accuracy: 0.7251
Epoch 9/25
102/102 - 0s - loss: 0.7841 - accuracy: 0.7360 - val_loss: 0.7939 - val_accuracy: 0.7361
Epoch 10/25
102/102 - 0s - loss: 0.7658 - accuracy: 0.7448 - val_loss: 0.7795 - val_accuracy: 0.7402
Epoch 11/25
102/102 - 0s - loss: 0.7487 - accuracy: 0.7485 - val_loss: 0.7641 - val_accuracy: 0.7413
Epoch 12/25
102/102 - 0s - loss: 0.7337 - accuracy: 0.7519 - val_loss: 0.7532 - val_accuracy: 0.7529
Epoch 13/25
102/102 - 0s - loss: 0.7203 - accuracy: 0.7593 - val_loss: 0.7389 - val_accuracy: 0.7488
Epoch 14/25
102/102 - 0s - loss: 0.7090 - accuracy: 0.7604 - val_loss: 0.7277 - val_accuracy: 0.7581
Epoch 15/25
102/102 - 0s - loss: 0.6960 - accuracy: 0.7643 - val_loss: 0.7213 - val_accuracy: 0.7645
Epoch 16/25
102/102 - 0s - loss: 0.6861 - accuracy: 0.7681 - val_loss: 0.7084 - val_accuracy: 0.7569
Epoch 17/25
102/102 - 0s - loss: 0.6733 - accuracy: 0.7757 - val_loss: 0.7021 - val_accuracy: 0.7581
Epoch 18/25
102/102 - 0s - loss: 0.6662 - accuracy: 0.7760 - val_loss: 0.6949 - val_accuracy: 0.7726
Epoch 19/25
102/102 - 0s - loss: 0.6552 - accuracy: 0.7811 - val_loss: 0.6800 - val_accuracy: 0.7703
Epoch 20/25
102/102 - 0s - loss: 0.6452 - accuracy: 0.7861 - val_loss: 0.6763 - val_accuracy: 0.7737
Epoch 21/25
102/102 - 0s - loss: 0.6375 - accuracy: 0.7899 - val_loss: 0.6658 - val_accuracy: 0.7737
Epoch 22/25
102/102 - 0s - loss: 0.6278 - accuracy: 0.7915 - val_loss: 0.6584 - val_accuracy: 0.7795
Epoch 23/25
102/102 - 0s - loss: 0.6189 - accuracy: 0.7955 - val_loss: 0.6479 - val_accuracy: 0.7824
Epoch 24/25
102/102 - 0s - loss: 0.6115 - accuracy: 0.7990 - val_loss: 0.6435 - val_accuracy: 0.7847
Epoch 25/25
102/102 - 0s - loss: 0.6037 - accuracy: 0.7984 - val_loss: 0.6358 - val_accuracy: 0.7836
6/6 - 0s - loss: 0.8523 - accuracy: 0.6343


Loss and metrics for TANH model with 64 batches and 25 epochs: 
Test Loss is 0.85 
Test Accuracy is 63.43 %

Epoch 1/40
102/102 - 1s - loss: 1.2656 - accuracy: 0.4453 - val_loss: 1.1503 - val_accuracy: 0.5492
Epoch 2/40
102/102 - 0s - loss: 1.0892 - accuracy: 0.5863 - val_loss: 1.0342 - val_accuracy: 0.6372
Epoch 3/40
102/102 - 0s - loss: 0.9954 - accuracy: 0.6508 - val_loss: 0.9667 - val_accuracy: 0.6771
Epoch 4/40
102/102 - 0s - loss: 0.9341 - accuracy: 0.6874 - val_loss: 0.9197 - val_accuracy: 0.6898
Epoch 5/40
102/102 - 0s - loss: 0.8907 - accuracy: 0.6977 - val_loss: 0.8834 - val_accuracy: 0.7112
Epoch 6/40
102/102 - 0s - loss: 0.8568 - accuracy: 0.7130 - val_loss: 0.8537 - val_accuracy: 0.7193
Epoch 7/40
102/102 - 0s - loss: 0.8292 - accuracy: 0.7189 - val_loss: 0.8323 - val_accuracy: 0.7188
Epoch 8/40
102/102 - 0s - loss: 0.8053 - accuracy: 0.7302 - val_loss: 0.8130 - val_accuracy: 0.7309
Epoch 9/40
102/102 - 0s - loss: 0.7859 - accuracy: 0.7357 - val_loss: 0.7944 - val_accuracy: 0.7309
Epoch 10/40
102/102 - 0s - loss: 0.7667 - accuracy: 0.7410 - val_loss: 0.7788 - val_accuracy: 0.7361
Epoch 11/40
102/102 - 0s - loss: 0.7508 - accuracy: 0.7482 - val_loss: 0.7666 - val_accuracy: 0.7402
Epoch 12/40
102/102 - 0s - loss: 0.7351 - accuracy: 0.7516 - val_loss: 0.7518 - val_accuracy: 0.7465
Epoch 13/40
102/102 - 0s - loss: 0.7217 - accuracy: 0.7583 - val_loss: 0.7407 - val_accuracy: 0.7517
Epoch 14/40
102/102 - 0s - loss: 0.7094 - accuracy: 0.7614 - val_loss: 0.7292 - val_accuracy: 0.7569
Epoch 15/40
102/102 - 0s - loss: 0.6965 - accuracy: 0.7648 - val_loss: 0.7181 - val_accuracy: 0.7593
Epoch 16/40
102/102 - 0s - loss: 0.6854 - accuracy: 0.7731 - val_loss: 0.7107 - val_accuracy: 0.7639
Epoch 17/40
102/102 - 0s - loss: 0.6750 - accuracy: 0.7726 - val_loss: 0.6981 - val_accuracy: 0.7616
Epoch 18/40
102/102 - 0s - loss: 0.6639 - accuracy: 0.7773 - val_loss: 0.6897 - val_accuracy: 0.7668
Epoch 19/40
102/102 - 0s - loss: 0.6555 - accuracy: 0.7825 - val_loss: 0.6797 - val_accuracy: 0.7679
Epoch 20/40
102/102 - 0s - loss: 0.6453 - accuracy: 0.7844 - val_loss: 0.6712 - val_accuracy: 0.7760
Epoch 21/40
102/102 - 0s - loss: 0.6357 - accuracy: 0.7875 - val_loss: 0.6689 - val_accuracy: 0.7726
Epoch 22/40
102/102 - 0s - loss: 0.6276 - accuracy: 0.7936 - val_loss: 0.6573 - val_accuracy: 0.7731
Epoch 23/40
102/102 - 0s - loss: 0.6201 - accuracy: 0.7952 - val_loss: 0.6508 - val_accuracy: 0.7784
Epoch 24/40
102/102 - 0s - loss: 0.6113 - accuracy: 0.8006 - val_loss: 0.6431 - val_accuracy: 0.7865
Epoch 25/40
102/102 - 0s - loss: 0.6030 - accuracy: 0.8003 - val_loss: 0.6369 - val_accuracy: 0.7853
Epoch 26/40
102/102 - 0s - loss: 0.5965 - accuracy: 0.8015 - val_loss: 0.6280 - val_accuracy: 0.7894
Epoch 27/40
102/102 - 0s - loss: 0.5894 - accuracy: 0.8049 - val_loss: 0.6212 - val_accuracy: 0.7888
Epoch 28/40
102/102 - 0s - loss: 0.5817 - accuracy: 0.8094 - val_loss: 0.6175 - val_accuracy: 0.7951
Epoch 29/40
102/102 - 0s - loss: 0.5751 - accuracy: 0.8099 - val_loss: 0.6082 - val_accuracy: 0.7963
Epoch 30/40
102/102 - 0s - loss: 0.5688 - accuracy: 0.8129 - val_loss: 0.6049 - val_accuracy: 0.8032
Epoch 31/40
102/102 - 0s - loss: 0.5611 - accuracy: 0.8143 - val_loss: 0.5958 - val_accuracy: 0.8003
Epoch 32/40
102/102 - 0s - loss: 0.5569 - accuracy: 0.8128 - val_loss: 0.5898 - val_accuracy: 0.8009
Epoch 33/40
102/102 - 0s - loss: 0.5504 - accuracy: 0.8184 - val_loss: 0.5883 - val_accuracy: 0.8056
Epoch 34/40
102/102 - 0s - loss: 0.5436 - accuracy: 0.8221 - val_loss: 0.5823 - val_accuracy: 0.8073
Epoch 35/40
102/102 - 0s - loss: 0.5384 - accuracy: 0.8238 - val_loss: 0.5764 - val_accuracy: 0.8096
Epoch 36/40
102/102 - 0s - loss: 0.5328 - accuracy: 0.8258 - val_loss: 0.5733 - val_accuracy: 0.8113
Epoch 37/40
102/102 - 0s - loss: 0.5274 - accuracy: 0.8276 - val_loss: 0.5664 - val_accuracy: 0.8125
Epoch 38/40
102/102 - 0s - loss: 0.5223 - accuracy: 0.8296 - val_loss: 0.5606 - val_accuracy: 0.8125
Epoch 39/40
102/102 - 0s - loss: 0.5178 - accuracy: 0.8310 - val_loss: 0.5542 - val_accuracy: 0.8189
Epoch 40/40
102/102 - 0s - loss: 0.5124 - accuracy: 0.8319 - val_loss: 0.5532 - val_accuracy: 0.8189
6/6 - 0s - loss: 0.8453 - accuracy: 0.6343


Loss and metrics for TANH model with 64 batches and 40 epochs: 
Test Loss is 0.85 
Test Accuracy is 63.43 %

Epoch 1/50
203/203 - 1s - loss: 1.2357 - accuracy: 0.4623 - val_loss: 1.0958 - val_accuracy: 0.6372
Epoch 2/50
203/203 - 1s - loss: 1.0318 - accuracy: 0.6610 - val_loss: 0.9700 - val_accuracy: 0.7176
Epoch 3/50
203/203 - 1s - loss: 0.9331 - accuracy: 0.7125 - val_loss: 0.8929 - val_accuracy: 0.7390
Epoch 4/50
203/203 - 1s - loss: 0.8661 - accuracy: 0.7403 - val_loss: 0.8412 - val_accuracy: 0.7483
Epoch 5/50
203/203 - 1s - loss: 0.8152 - accuracy: 0.7615 - val_loss: 0.7974 - val_accuracy: 0.7645
Epoch 6/50
203/203 - 1s - loss: 0.7737 - accuracy: 0.7729 - val_loss: 0.7627 - val_accuracy: 0.7789
Epoch 7/50
203/203 - 1s - loss: 0.7369 - accuracy: 0.7875 - val_loss: 0.7282 - val_accuracy: 0.7870
Epoch 8/50
203/203 - 1s - loss: 0.7036 - accuracy: 0.7929 - val_loss: 0.6969 - val_accuracy: 0.7922
Epoch 9/50
203/203 - 1s - loss: 0.6743 - accuracy: 0.8006 - val_loss: 0.6726 - val_accuracy: 0.7975
Epoch 10/50
203/203 - 1s - loss: 0.6488 - accuracy: 0.8052 - val_loss: 0.6478 - val_accuracy: 0.8067
Epoch 11/50
203/203 - 1s - loss: 0.6244 - accuracy: 0.8146 - val_loss: 0.6271 - val_accuracy: 0.8073
Epoch 12/50
203/203 - 1s - loss: 0.6023 - accuracy: 0.8234 - val_loss: 0.6075 - val_accuracy: 0.8177
Epoch 13/50
203/203 - 1s - loss: 0.5842 - accuracy: 0.8244 - val_loss: 0.5973 - val_accuracy: 0.8177
Epoch 14/50
203/203 - 1s - loss: 0.5659 - accuracy: 0.8265 - val_loss: 0.5758 - val_accuracy: 0.8270
Epoch 15/50
203/203 - 1s - loss: 0.5488 - accuracy: 0.8329 - val_loss: 0.5610 - val_accuracy: 0.8304
Epoch 16/50
203/203 - 1s - loss: 0.5340 - accuracy: 0.8409 - val_loss: 0.5471 - val_accuracy: 0.8316
Epoch 17/50
203/203 - 1s - loss: 0.5201 - accuracy: 0.8420 - val_loss: 0.5331 - val_accuracy: 0.8368
Epoch 18/50
203/203 - 1s - loss: 0.5067 - accuracy: 0.8463 - val_loss: 0.5243 - val_accuracy: 0.8374
Epoch 19/50
203/203 - 1s - loss: 0.4945 - accuracy: 0.8506 - val_loss: 0.5137 - val_accuracy: 0.8414
Epoch 20/50
203/203 - 1s - loss: 0.4839 - accuracy: 0.8503 - val_loss: 0.5044 - val_accuracy: 0.8501
Epoch 21/50
203/203 - 1s - loss: 0.4728 - accuracy: 0.8562 - val_loss: 0.4979 - val_accuracy: 0.8519
Epoch 22/50
203/203 - 1s - loss: 0.4633 - accuracy: 0.8584 - val_loss: 0.4831 - val_accuracy: 0.8513
Epoch 23/50
203/203 - 1s - loss: 0.4540 - accuracy: 0.8590 - val_loss: 0.4748 - val_accuracy: 0.8519
Epoch 24/50
203/203 - 1s - loss: 0.4447 - accuracy: 0.8631 - val_loss: 0.4681 - val_accuracy: 0.8576
Epoch 25/50
203/203 - 1s - loss: 0.4365 - accuracy: 0.8669 - val_loss: 0.4635 - val_accuracy: 0.8588
Epoch 26/50
203/203 - 1s - loss: 0.4288 - accuracy: 0.8681 - val_loss: 0.4551 - val_accuracy: 0.8582
Epoch 27/50
203/203 - 1s - loss: 0.4211 - accuracy: 0.8687 - val_loss: 0.4480 - val_accuracy: 0.8634
Epoch 28/50
203/203 - 1s - loss: 0.4144 - accuracy: 0.8686 - val_loss: 0.4415 - val_accuracy: 0.8623
Epoch 29/50
203/203 - 1s - loss: 0.4067 - accuracy: 0.8744 - val_loss: 0.4355 - val_accuracy: 0.8663
Epoch 30/50
203/203 - 1s - loss: 0.4008 - accuracy: 0.8752 - val_loss: 0.4291 - val_accuracy: 0.8686
Epoch 31/50
203/203 - 1s - loss: 0.3944 - accuracy: 0.8770 - val_loss: 0.4268 - val_accuracy: 0.8675
Epoch 32/50
203/203 - 1s - loss: 0.3884 - accuracy: 0.8757 - val_loss: 0.4238 - val_accuracy: 0.8704
Epoch 33/50
203/203 - 1s - loss: 0.3827 - accuracy: 0.8801 - val_loss: 0.4176 - val_accuracy: 0.8715
Epoch 34/50
203/203 - 1s - loss: 0.3775 - accuracy: 0.8800 - val_loss: 0.4092 - val_accuracy: 0.8738
Epoch 35/50
203/203 - 1s - loss: 0.3727 - accuracy: 0.8817 - val_loss: 0.4053 - val_accuracy: 0.8715
Epoch 36/50
203/203 - 1s - loss: 0.3677 - accuracy: 0.8852 - val_loss: 0.4007 - val_accuracy: 0.8738
Epoch 37/50
203/203 - 1s - loss: 0.3632 - accuracy: 0.8838 - val_loss: 0.3966 - val_accuracy: 0.8744
Epoch 38/50
203/203 - 1s - loss: 0.3582 - accuracy: 0.8857 - val_loss: 0.3958 - val_accuracy: 0.8779
Epoch 39/50
203/203 - 1s - loss: 0.3540 - accuracy: 0.8896 - val_loss: 0.3883 - val_accuracy: 0.8814
Epoch 40/50
203/203 - 1s - loss: 0.3497 - accuracy: 0.8896 - val_loss: 0.3840 - val_accuracy: 0.8831
Epoch 41/50
203/203 - 1s - loss: 0.3455 - accuracy: 0.8899 - val_loss: 0.3825 - val_accuracy: 0.8819
Epoch 42/50
203/203 - 1s - loss: 0.3423 - accuracy: 0.8913 - val_loss: 0.3777 - val_accuracy: 0.8837
Epoch 43/50
203/203 - 1s - loss: 0.3371 - accuracy: 0.8931 - val_loss: 0.3744 - val_accuracy: 0.8872
Epoch 44/50
203/203 - 1s - loss: 0.3343 - accuracy: 0.8936 - val_loss: 0.3719 - val_accuracy: 0.8866
Epoch 45/50
203/203 - 1s - loss: 0.3298 - accuracy: 0.8948 - val_loss: 0.3716 - val_accuracy: 0.8843
Epoch 46/50
203/203 - 1s - loss: 0.3271 - accuracy: 0.8950 - val_loss: 0.3658 - val_accuracy: 0.8872
Epoch 47/50
203/203 - 1s - loss: 0.3238 - accuracy: 0.8981 - val_loss: 0.3613 - val_accuracy: 0.8889
Epoch 48/50
203/203 - 1s - loss: 0.3208 - accuracy: 0.8990 - val_loss: 0.3587 - val_accuracy: 0.8854
Epoch 49/50
203/203 - 1s - loss: 0.3164 - accuracy: 0.9001 - val_loss: 0.3588 - val_accuracy: 0.8929
Epoch 50/50
203/203 - 1s - loss: 0.3141 - accuracy: 0.9005 - val_loss: 0.3649 - val_accuracy: 0.8860
6/6 - 0s - loss: 0.8506 - accuracy: 0.6971


Loss and metrics for RELU model with 32 batches and 50 epochs: 
Test Loss is 0.85 
Test Accuracy is 69.71 %

Epoch 1/50
203/203 - 1s - loss: 1.1963 - accuracy: 0.5102 - val_loss: 1.0139 - val_accuracy: 0.5660
Epoch 2/50
203/203 - 1s - loss: 0.7668 - accuracy: 0.7121 - val_loss: 0.7205 - val_accuracy: 0.7425
Epoch 3/50
203/203 - 1s - loss: 0.6351 - accuracy: 0.7722 - val_loss: 0.6121 - val_accuracy: 0.7847
Epoch 4/50
203/203 - 1s - loss: 0.5727 - accuracy: 0.7977 - val_loss: 0.5586 - val_accuracy: 0.8119
Epoch 5/50
203/203 - 1s - loss: 0.5215 - accuracy: 0.8216 - val_loss: 0.5055 - val_accuracy: 0.8293
Epoch 6/50
203/203 - 1s - loss: 0.4831 - accuracy: 0.8398 - val_loss: 0.4810 - val_accuracy: 0.8466
Epoch 7/50
203/203 - 1s - loss: 0.4609 - accuracy: 0.8508 - val_loss: 0.4390 - val_accuracy: 0.8547
Epoch 8/50
203/203 - 1s - loss: 0.4421 - accuracy: 0.8557 - val_loss: 0.4418 - val_accuracy: 0.8571
Epoch 9/50
203/203 - 1s - loss: 0.4252 - accuracy: 0.8591 - val_loss: 0.4060 - val_accuracy: 0.8686
Epoch 10/50
203/203 - 1s - loss: 0.4097 - accuracy: 0.8638 - val_loss: 0.4023 - val_accuracy: 0.8709
Epoch 11/50
203/203 - 1s - loss: 0.3978 - accuracy: 0.8729 - val_loss: 0.3888 - val_accuracy: 0.8750
Epoch 12/50
203/203 - 1s - loss: 0.3820 - accuracy: 0.8760 - val_loss: 0.3831 - val_accuracy: 0.8756
Epoch 13/50
203/203 - 1s - loss: 0.3701 - accuracy: 0.8772 - val_loss: 0.3812 - val_accuracy: 0.8808
Epoch 14/50
203/203 - 1s - loss: 0.3716 - accuracy: 0.8792 - val_loss: 0.3791 - val_accuracy: 0.8808
Epoch 15/50
203/203 - 1s - loss: 0.3584 - accuracy: 0.8848 - val_loss: 0.3565 - val_accuracy: 0.8889
Epoch 16/50
203/203 - 1s - loss: 0.3429 - accuracy: 0.8868 - val_loss: 0.3509 - val_accuracy: 0.8912
Epoch 17/50
203/203 - 1s - loss: 0.3383 - accuracy: 0.8911 - val_loss: 0.3550 - val_accuracy: 0.8843
Epoch 18/50
203/203 - 1s - loss: 0.3318 - accuracy: 0.8953 - val_loss: 0.3433 - val_accuracy: 0.8900
Epoch 19/50
203/203 - 1s - loss: 0.3235 - accuracy: 0.8979 - val_loss: 0.3397 - val_accuracy: 0.8900
Epoch 20/50
203/203 - 1s - loss: 0.3236 - accuracy: 0.8962 - val_loss: 0.3471 - val_accuracy: 0.8895
Epoch 21/50
203/203 - 1s - loss: 0.3205 - accuracy: 0.8971 - val_loss: 0.3365 - val_accuracy: 0.8929
Epoch 22/50
203/203 - 1s - loss: 0.3106 - accuracy: 0.9025 - val_loss: 0.3317 - val_accuracy: 0.8918
Epoch 23/50
203/203 - 1s - loss: 0.3090 - accuracy: 0.9027 - val_loss: 0.3265 - val_accuracy: 0.8941
Epoch 24/50
203/203 - 1s - loss: 0.3001 - accuracy: 0.9010 - val_loss: 0.3283 - val_accuracy: 0.8924
Epoch 25/50
203/203 - 1s - loss: 0.2986 - accuracy: 0.9059 - val_loss: 0.3248 - val_accuracy: 0.8953
Epoch 26/50
203/203 - 1s - loss: 0.2868 - accuracy: 0.9086 - val_loss: 0.3122 - val_accuracy: 0.8987
Epoch 27/50
203/203 - 1s - loss: 0.2862 - accuracy: 0.9093 - val_loss: 0.3272 - val_accuracy: 0.8970
Epoch 28/50
203/203 - 1s - loss: 0.2826 - accuracy: 0.9087 - val_loss: 0.3122 - val_accuracy: 0.9005
Epoch 29/50
203/203 - 1s - loss: 0.2782 - accuracy: 0.9099 - val_loss: 0.3443 - val_accuracy: 0.8848
Epoch 30/50
203/203 - 1s - loss: 0.2729 - accuracy: 0.9132 - val_loss: 0.3155 - val_accuracy: 0.8999
Epoch 31/50
203/203 - 1s - loss: 0.2720 - accuracy: 0.9147 - val_loss: 0.3280 - val_accuracy: 0.8953
Epoch 32/50
203/203 - 1s - loss: 0.2676 - accuracy: 0.9160 - val_loss: 0.3251 - val_accuracy: 0.8941
Epoch 33/50
203/203 - 1s - loss: 0.2631 - accuracy: 0.9183 - val_loss: 0.2987 - val_accuracy: 0.8987
Epoch 34/50
203/203 - 1s - loss: 0.2653 - accuracy: 0.9172 - val_loss: 0.3017 - val_accuracy: 0.9045
Epoch 35/50
203/203 - 1s - loss: 0.2579 - accuracy: 0.9174 - val_loss: 0.2927 - val_accuracy: 0.9062
Epoch 36/50
203/203 - 1s - loss: 0.2545 - accuracy: 0.9194 - val_loss: 0.3332 - val_accuracy: 0.8935
Epoch 37/50
203/203 - 1s - loss: 0.2501 - accuracy: 0.9212 - val_loss: 0.3011 - val_accuracy: 0.9057
Epoch 38/50
203/203 - 1s - loss: 0.2487 - accuracy: 0.9225 - val_loss: 0.2918 - val_accuracy: 0.9097
Epoch 39/50
203/203 - 1s - loss: 0.2478 - accuracy: 0.9215 - val_loss: 0.2860 - val_accuracy: 0.9068
Epoch 40/50
203/203 - 1s - loss: 0.2365 - accuracy: 0.9276 - val_loss: 0.2854 - val_accuracy: 0.9068
Epoch 41/50
203/203 - 1s - loss: 0.2387 - accuracy: 0.9293 - val_loss: 0.3104 - val_accuracy: 0.9010
Epoch 42/50
203/203 - 1s - loss: 0.2296 - accuracy: 0.9302 - val_loss: 0.2928 - val_accuracy: 0.9068
Epoch 43/50
203/203 - 1s - loss: 0.2327 - accuracy: 0.9269 - val_loss: 0.2839 - val_accuracy: 0.9074
Epoch 44/50
203/203 - 1s - loss: 0.2340 - accuracy: 0.9269 - val_loss: 0.2922 - val_accuracy: 0.9034
Epoch 45/50
203/203 - 1s - loss: 0.2283 - accuracy: 0.9288 - val_loss: 0.2819 - val_accuracy: 0.9062
Epoch 46/50
203/203 - 1s - loss: 0.2281 - accuracy: 0.9291 - val_loss: 0.2868 - val_accuracy: 0.9091
Epoch 47/50
203/203 - 1s - loss: 0.2243 - accuracy: 0.9285 - val_loss: 0.3053 - val_accuracy: 0.9034
Epoch 48/50
203/203 - 1s - loss: 0.2186 - accuracy: 0.9336 - val_loss: 0.3216 - val_accuracy: 0.8976
Epoch 49/50
203/203 - 1s - loss: 0.2222 - accuracy: 0.9297 - val_loss: 0.2879 - val_accuracy: 0.9080
Epoch 50/50
203/203 - 1s - loss: 0.2166 - accuracy: 0.9357 - val_loss: 0.2777 - val_accuracy: 0.9103
6/6 - 0s - loss: 0.9357 - accuracy: 0.6229


Loss and metrics for TANH model with batch normalization, 32 batches and 50 epochs: 
Test Loss is 0.94 
Test Accuracy is 62.29 %

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
permute (Permute)            (None, 32, 32, 1)         0         
_________________________________________________________________
Conv2D_1 (Conv2D)            (None, 32, 32, 64)        640       
_________________________________________________________________
dropout (Dropout)            (None, 32, 32, 64)        0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 32, 32, 64)        128       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         
_________________________________________________________________
Conv2D_2 (Conv2D)            (None, 16, 16, 128)       73856     
_________________________________________________________________
dropout_1 (Dropout)          (None, 16, 16, 128)       0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 16, 16, 128)       64        
_________________________________________________________________
Conv2D_3 (Conv2D)            (None, 16, 16, 128)       147584    
_________________________________________________________________
dropout_2 (Dropout)          (None, 16, 16, 128)       0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 16, 16, 128)       64        
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         
_________________________________________________________________
Conv2D_4 (Conv2D)            (None, 8, 8, 256)         295168    
_________________________________________________________________
dropout_3 (Dropout)          (None, 8, 8, 256)         0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 8, 8, 256)         32        
_________________________________________________________________
Conv2D_5 (Conv2D)            (None, 8, 8, 256)         590080    
_________________________________________________________________
dropout_4 (Dropout)          (None, 8, 8, 256)         0         
_________________________________________________________________
batch_normalization_6 (Batch (None, 8, 8, 256)         32        
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4096)              0         
_________________________________________________________________
FCN_1 (Dense)                (None, 512)               2097664   
_________________________________________________________________
dropout_5 (Dropout)          (None, 512)               0         
_________________________________________________________________
FCN_2 (Dense)                (None, 128)               65664     
_________________________________________________________________
FCN_3 (Dense)                (None, 4)                 516       
=================================================================
Total params: 3,271,492
Trainable params: 3,271,332
Non-trainable params: 160
_________________________________________________________________
Epoch 1/25
102/102 [==============================] - 17s 161ms/step - loss: 8.1857 - acc: 0.3802 - val_loss: 7.9092 - val_acc: 0.2500
Epoch 2/25
102/102 [==============================] - 16s 157ms/step - loss: 7.5819 - acc: 0.5379 - val_loss: 8.0729 - val_acc: 0.2500
Epoch 3/25
102/102 [==============================] - 16s 156ms/step - loss: 7.4060 - acc: 0.5966 - val_loss: 8.1678 - val_acc: 0.2500
Epoch 4/25
102/102 [==============================] - 16s 154ms/step - loss: 7.2983 - acc: 0.6292 - val_loss: 8.1782 - val_acc: 0.2703
Epoch 5/25
102/102 [==============================] - 16s 154ms/step - loss: 7.2265 - acc: 0.6423 - val_loss: 7.9822 - val_acc: 0.2969
Epoch 6/25
102/102 [==============================] - 16s 156ms/step - loss: 7.1347 - acc: 0.6792 - val_loss: 7.7833 - val_acc: 0.3576
Epoch 7/25
102/102 [==============================] - 16s 156ms/step - loss: 7.0829 - acc: 0.6816 - val_loss: 7.6691 - val_acc: 0.4358
Epoch 8/25
102/102 [==============================] - 16s 157ms/step - loss: 6.9932 - acc: 0.7072 - val_loss: 7.5568 - val_acc: 0.4606
Epoch 9/25
102/102 [==============================] - 16s 159ms/step - loss: 6.9454 - acc: 0.7188 - val_loss: 7.5193 - val_acc: 0.4716
Epoch 10/25
102/102 [==============================] - 16s 158ms/step - loss: 6.9254 - acc: 0.7204 - val_loss: 7.5275 - val_acc: 0.4734
Epoch 11/25
102/102 [==============================] - 16s 158ms/step - loss: 6.8736 - acc: 0.7389 - val_loss: 7.5137 - val_acc: 0.4728
Epoch 12/25
102/102 [==============================] - 16s 159ms/step - loss: 6.8292 - acc: 0.7494 - val_loss: 7.5152 - val_acc: 0.4670
Epoch 13/25
102/102 [==============================] - 16s 158ms/step - loss: 6.7896 - acc: 0.7554 - val_loss: 7.4944 - val_acc: 0.4711
Epoch 14/25
102/102 [==============================] - 16s 158ms/step - loss: 6.7641 - acc: 0.7552 - val_loss: 7.4979 - val_acc: 0.4653
Epoch 15/25
102/102 [==============================] - 16s 158ms/step - loss: 6.7242 - acc: 0.7683 - val_loss: 7.4951 - val_acc: 0.4682
Epoch 16/25
102/102 [==============================] - 16s 155ms/step - loss: 6.7025 - acc: 0.7662 - val_loss: 7.5013 - val_acc: 0.4606
Epoch 17/25
102/102 [==============================] - 16s 159ms/step - loss: 6.6675 - acc: 0.7757 - val_loss: 7.4827 - val_acc: 0.4578
Epoch 18/25
102/102 [==============================] - 16s 159ms/step - loss: 6.5986 - acc: 0.7944 - val_loss: 7.4790 - val_acc: 0.4566
Epoch 19/25
102/102 [==============================] - 16s 158ms/step - loss: 6.6086 - acc: 0.7797 - val_loss: 7.4573 - val_acc: 0.4497
Epoch 20/25
102/102 [==============================] - 16s 158ms/step - loss: 6.5562 - acc: 0.7973 - val_loss: 7.4295 - val_acc: 0.4653
Epoch 21/25
102/102 [==============================] - 16s 160ms/step - loss: 6.5709 - acc: 0.7860 - val_loss: 7.4483 - val_acc: 0.4491
Epoch 22/25
102/102 [==============================] - 16s 158ms/step - loss: 6.5087 - acc: 0.8039 - val_loss: 7.4605 - val_acc: 0.4531
Epoch 23/25
102/102 [==============================] - 16s 158ms/step - loss: 6.4770 - acc: 0.8053 - val_loss: 7.4556 - val_acc: 0.4554
Epoch 24/25
102/102 [==============================] - 16s 158ms/step - loss: 6.4654 - acc: 0.8089 - val_loss: 7.4316 - val_acc: 0.4630
Epoch 25/25
102/102 [==============================] - 16s 158ms/step - loss: 6.4507 - acc: 0.8028 - val_loss: 7.4020 - val_acc: 0.4693
6/6 [==============================] - 0s 17ms/step - loss: 7.8548 - acc: 0.3200
test loss, test acc: [7.854780197143555, 0.3199999928474426]
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
permute_1 (Permute)          (None, 32, 32, 1)         0         
_________________________________________________________________
Conv2D_1 (Conv2D)            (None, 32, 32, 32)        320       
_________________________________________________________________
dropout_6 (Dropout)          (None, 32, 32, 32)        0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 32, 32, 32)        128       
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 16, 16, 32)        0         
_________________________________________________________________
Conv2D_2 (Conv2D)            (None, 16, 16, 64)        18496     
_________________________________________________________________
dropout_7 (Dropout)          (None, 16, 16, 64)        0         
_________________________________________________________________
batch_normalization_8 (Batch (None, 16, 16, 64)        64        
_________________________________________________________________
Conv2D_3 (Conv2D)            (None, 16, 16, 64)        36928     
_________________________________________________________________
dropout_8 (Dropout)          (None, 16, 16, 64)        0         
_________________________________________________________________
batch_normalization_9 (Batch (None, 16, 16, 64)        64        
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 8, 8, 64)          0         
_________________________________________________________________
Conv2D_4 (Conv2D)            (None, 8, 8, 128)         73856     
_________________________________________________________________
dropout_9 (Dropout)          (None, 8, 8, 128)         0         
_________________________________________________________________
batch_normalization_10 (Batc (None, 8, 8, 128)         32        
_________________________________________________________________
Conv2D_5 (Conv2D)            (None, 8, 8, 128)         147584    
_________________________________________________________________
dropout_10 (Dropout)         (None, 8, 8, 128)         0         
_________________________________________________________________
batch_normalization_11 (Batc (None, 8, 8, 128)         32        
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 4, 4, 128)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 2048)              0         
_________________________________________________________________
FCN_1 (Dense)                (None, 512)               1049088   
_________________________________________________________________
dropout_11 (Dropout)         (None, 512)               0         
_________________________________________________________________
FCN_2 (Dense)                (None, 128)               65664     
_________________________________________________________________
FCN_3 (Dense)                (None, 4)                 516       
=================================================================
Total params: 1,392,772
Trainable params: 1,392,612
Non-trainable params: 160
_________________________________________________________________
Epoch 1/25
102/102 [==============================] - 9s 77ms/step - loss: 5.2499 - acc: 0.3307 - val_loss: 4.7211 - val_acc: 0.2500
Epoch 2/25
102/102 [==============================] - 8s 74ms/step - loss: 4.5673 - acc: 0.4893 - val_loss: 4.8470 - val_acc: 0.2500
Epoch 3/25
102/102 [==============================] - 7s 73ms/step - loss: 4.4761 - acc: 0.5101 - val_loss: 4.8791 - val_acc: 0.2500
Epoch 4/25
102/102 [==============================] - 8s 74ms/step - loss: 4.3583 - acc: 0.5436 - val_loss: 4.7965 - val_acc: 0.2506
Epoch 5/25
102/102 [==============================] - 8s 74ms/step - loss: 4.3274 - acc: 0.5588 - val_loss: 4.6865 - val_acc: 0.2627
Epoch 6/25
102/102 [==============================] - 8s 75ms/step - loss: 4.2890 - acc: 0.5794 - val_loss: 4.5898 - val_acc: 0.3183
Epoch 7/25
102/102 [==============================] - 8s 74ms/step - loss: 4.2449 - acc: 0.5925 - val_loss: 4.5311 - val_acc: 0.3640
Epoch 8/25
102/102 [==============================] - 7s 73ms/step - loss: 4.2465 - acc: 0.5885 - val_loss: 4.5113 - val_acc: 0.3906
Epoch 9/25
102/102 [==============================] - 7s 73ms/step - loss: 4.1930 - acc: 0.5932 - val_loss: 4.5014 - val_acc: 0.4016
Epoch 10/25
102/102 [==============================] - 7s 73ms/step - loss: 4.1887 - acc: 0.6025 - val_loss: 4.5066 - val_acc: 0.3964
Epoch 11/25
102/102 [==============================] - 7s 73ms/step - loss: 4.1197 - acc: 0.6249 - val_loss: 4.5047 - val_acc: 0.4005
Epoch 12/25
102/102 [==============================] - 7s 73ms/step - loss: 4.1072 - acc: 0.6208 - val_loss: 4.5017 - val_acc: 0.4016
Epoch 13/25
102/102 [==============================] - 7s 73ms/step - loss: 4.1360 - acc: 0.6177 - val_loss: 4.5009 - val_acc: 0.4022
Epoch 14/25
102/102 [==============================] - 7s 73ms/step - loss: 4.1071 - acc: 0.6352 - val_loss: 4.5088 - val_acc: 0.3987
Epoch 15/25
102/102 [==============================] - 7s 73ms/step - loss: 4.0928 - acc: 0.6301 - val_loss: 4.5073 - val_acc: 0.3987
Epoch 16/25
102/102 [==============================] - 8s 74ms/step - loss: 4.0756 - acc: 0.6434 - val_loss: 4.5004 - val_acc: 0.4115
Epoch 17/25
102/102 [==============================] - 8s 74ms/step - loss: 4.0570 - acc: 0.6507 - val_loss: 4.4997 - val_acc: 0.4120
Epoch 18/25
102/102 [==============================] - 7s 73ms/step - loss: 4.0219 - acc: 0.6629 - val_loss: 4.4997 - val_acc: 0.4103
Epoch 19/25
102/102 [==============================] - 7s 74ms/step - loss: 4.0467 - acc: 0.6419 - val_loss: 4.5055 - val_acc: 0.4120
Epoch 20/25
102/102 [==============================] - 8s 74ms/step - loss: 4.0159 - acc: 0.6631 - val_loss: 4.5021 - val_acc: 0.4138
Epoch 21/25
102/102 [==============================] - 7s 72ms/step - loss: 4.0016 - acc: 0.6599 - val_loss: 4.5070 - val_acc: 0.4039
Epoch 22/25
102/102 [==============================] - 7s 73ms/step - loss: 4.0151 - acc: 0.6559 - val_loss: 4.4991 - val_acc: 0.4132
Epoch 23/25
102/102 [==============================] - 7s 73ms/step - loss: 3.9848 - acc: 0.6618 - val_loss: 4.4990 - val_acc: 0.4086
Epoch 24/25
102/102 [==============================] - 7s 73ms/step - loss: 3.9820 - acc: 0.6694 - val_loss: 4.5008 - val_acc: 0.4057
Epoch 25/25
102/102 [==============================] - 7s 73ms/step - loss: 3.9514 - acc: 0.6846 - val_loss: 4.4999 - val_acc: 0.4126
6/6 [==============================] - 0s 10ms/step - loss: 4.8100 - acc: 0.3257
test loss, test acc: [4.809974193572998, 0.3257142901420593]

All the results we got are:

                  RELU (64 batches, 25 epochs)  TANH (64 batches, 25 epochs)  \
Test loss                             0.800189                      0.852267   
Test accuracy(%)                     69.142854                     63.428569   

                  TANH (64 batches, 40 epochs)  RELU (32 batches, 50 epochs)  \
Test loss                             0.845267                      0.850554   
Test accuracy(%)                     63.428569                     69.714284   

                  TANH (batch normalized, 32 batches, 50 epochs)  \
Test loss                                               0.935657   
Test accuracy(%)                                       62.285715   

                  NNet ([64,128,128,256,256] filters)  \
Test loss                                    7.854780   
Test accuracy(%)                            31.999999   

                  NNet ([32,64,64,128,128] filters)  
Test loss                                  4.809974  
Test accuracy(%)                          32.571429  


(2ndPaper-t2) stu4@triton01:~/HW4_yoel_amit$ 
