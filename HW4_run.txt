
(2ndPaper-t2) stu4@triton01:~/HW4_yoel_amit$ python HW4.py
/opt/miniconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/opt/miniconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/opt/miniconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/opt/miniconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/opt/miniconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/opt/miniconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2021-02-24 22:34:44.834669: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2021-02-24 22:34:44.986133: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55e45f460640 executing computations on platform CUDA. Devices:
2021-02-24 22:34:44.986220: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2080, Compute Capability 7.5
2021-02-24 22:34:44.994291: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2021-02-24 22:34:45.002613: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55e45f432e10 executing computations on platform Host. Devices:
2021-02-24 22:34:45.002668: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2021-02-24 22:34:45.003373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.71
pciBusID: 0000:5e:00.0
totalMemory: 7.77GiB freeMemory: 3.99GiB
2021-02-24 22:34:45.003412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2021-02-24 22:34:45.005578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-02-24 22:34:45.005620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2021-02-24 22:34:45.005631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2021-02-24 22:34:45.006106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6361 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:5e:00.0, compute capability: 7.5)
WARNING:tensorflow:From /opt/miniconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 1024)              0         
_________________________________________________________________
dense (Dense)                (None, 300)               307500    
_________________________________________________________________
Relu1 (Activation)           (None, 300)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 150)               45150     
_________________________________________________________________
Relu2 (Activation)           (None, 150)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 4)                 604       
_________________________________________________________________
activation (Activation)      (None, 4)                 0         
=================================================================
Total params: 353,254
Trainable params: 353,254
Non-trainable params: 0
_________________________________________________________________
Train on 6474 samples, validate on 1728 samples
WARNING:tensorflow:From /opt/miniconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2021-02-24 22:35:00.387857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2021-02-24 22:35:00.387934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-02-24 22:35:00.387944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2021-02-24 22:35:00.387948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2021-02-24 22:35:00.388254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6361 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:5e:00.0, compute capability: 7.5)
Epoch 1/25
2021-02-24 22:35:01.185920: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
 - 1s - loss: 1.3030 - acc: 0.4439 - val_loss: 1.2019 - val_acc: 0.5405
Epoch 2/25
 - 1s - loss: 1.1544 - acc: 0.5721 - val_loss: 1.1046 - val_acc: 0.5897
Epoch 3/25
 - 1s - loss: 1.0758 - acc: 0.6222 - val_loss: 1.0397 - val_acc: 0.6279
Epoch 4/25
 - 1s - loss: 1.0157 - acc: 0.6647 - val_loss: 0.9862 - val_acc: 0.6852
Epoch 5/25
 - 1s - loss: 0.9651 - acc: 0.6999 - val_loss: 0.9428 - val_acc: 0.7112
Epoch 6/25
 - 1s - loss: 0.9249 - acc: 0.7153 - val_loss: 0.9072 - val_acc: 0.7211
Epoch 7/25
 - 1s - loss: 0.8901 - acc: 0.7269 - val_loss: 0.8752 - val_acc: 0.7309
Epoch 8/25
 - 1s - loss: 0.8565 - acc: 0.7416 - val_loss: 0.8451 - val_acc: 0.7413
Epoch 9/25
 - 1s - loss: 0.8252 - acc: 0.7510 - val_loss: 0.8159 - val_acc: 0.7413
Epoch 10/25
 - 1s - loss: 0.7992 - acc: 0.7567 - val_loss: 0.7928 - val_acc: 0.7575
Epoch 11/25
 - 1s - loss: 0.7749 - acc: 0.7663 - val_loss: 0.7715 - val_acc: 0.7587
Epoch 12/25
 - 1s - loss: 0.7524 - acc: 0.7722 - val_loss: 0.7484 - val_acc: 0.7622
Epoch 13/25
 - 1s - loss: 0.7283 - acc: 0.7762 - val_loss: 0.7274 - val_acc: 0.7656
Epoch 14/25
 - 1s - loss: 0.7096 - acc: 0.7813 - val_loss: 0.7112 - val_acc: 0.7807
Epoch 15/25
 - 1s - loss: 0.6920 - acc: 0.7882 - val_loss: 0.6952 - val_acc: 0.7812
Epoch 16/25
 - 1s - loss: 0.6759 - acc: 0.7935 - val_loss: 0.6828 - val_acc: 0.7882
Epoch 17/25
 - 1s - loss: 0.6621 - acc: 0.7970 - val_loss: 0.6674 - val_acc: 0.7917
Epoch 18/25
 - 1s - loss: 0.6472 - acc: 0.8009 - val_loss: 0.6535 - val_acc: 0.7975
Epoch 19/25
 - 1s - loss: 0.6338 - acc: 0.8058 - val_loss: 0.6418 - val_acc: 0.8044
Epoch 20/25
 - 1s - loss: 0.6215 - acc: 0.8123 - val_loss: 0.6294 - val_acc: 0.8061
Epoch 21/25
 - 1s - loss: 0.6093 - acc: 0.8154 - val_loss: 0.6188 - val_acc: 0.8102
Epoch 22/25
 - 1s - loss: 0.5972 - acc: 0.8184 - val_loss: 0.6062 - val_acc: 0.8148
Epoch 23/25
 - 1s - loss: 0.5854 - acc: 0.8214 - val_loss: 0.5955 - val_acc: 0.8177
Epoch 24/25
 - 1s - loss: 0.5748 - acc: 0.8241 - val_loss: 0.5864 - val_acc: 0.8177
Epoch 25/25
 - 1s - loss: 0.5652 - acc: 0.8242 - val_loss: 0.5760 - val_acc: 0.8200
 - 0s - loss: 0.8045 - acc: 0.6686


Loss and metrics for RELU model with 64 batches and 25 epochs: 
Test Loss is 0.80 
Test Accuracy is 66.86 %

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense_3 (Dense)              (None, 300)               307500    
_________________________________________________________________
tanh1 (Activation)           (None, 300)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 150)               45150     
_________________________________________________________________
tanh2 (Activation)           (None, 150)               0         
_________________________________________________________________
dense_5 (Dense)              (None, 4)                 604       
_________________________________________________________________
activation_1 (Activation)    (None, 4)                 0         
=================================================================
Total params: 353,254
Trainable params: 353,254
Non-trainable params: 0
_________________________________________________________________
Train on 6474 samples, validate on 1728 samples
Epoch 1/25
 - 1s - loss: 1.2836 - acc: 0.4551 - val_loss: 1.1937 - val_acc: 0.5741
Epoch 2/25
 - 1s - loss: 1.1310 - acc: 0.5984 - val_loss: 1.0873 - val_acc: 0.6325
Epoch 3/25
 - 1s - loss: 1.0448 - acc: 0.6393 - val_loss: 1.0193 - val_acc: 0.6713
Epoch 4/25
 - 1s - loss: 0.9862 - acc: 0.6650 - val_loss: 0.9729 - val_acc: 0.6834
Epoch 5/25
 - 1s - loss: 0.9431 - acc: 0.6829 - val_loss: 0.9387 - val_acc: 0.6927
Epoch 6/25
 - 1s - loss: 0.9086 - acc: 0.6985 - val_loss: 0.9090 - val_acc: 0.6921
Epoch 7/25
 - 1s - loss: 0.8804 - acc: 0.7022 - val_loss: 0.8832 - val_acc: 0.7066
Epoch 8/25
 - 1s - loss: 0.8550 - acc: 0.7150 - val_loss: 0.8620 - val_acc: 0.7135
Epoch 9/25
 - 1s - loss: 0.8321 - acc: 0.7178 - val_loss: 0.8434 - val_acc: 0.7216
Epoch 10/25
 - 1s - loss: 0.8120 - acc: 0.7251 - val_loss: 0.8269 - val_acc: 0.7263
Epoch 11/25
 - 1s - loss: 0.7948 - acc: 0.7305 - val_loss: 0.8100 - val_acc: 0.7216
Epoch 12/25
 - 1s - loss: 0.7789 - acc: 0.7365 - val_loss: 0.7965 - val_acc: 0.7321
Epoch 13/25
 - 1s - loss: 0.7638 - acc: 0.7405 - val_loss: 0.7827 - val_acc: 0.7396
Epoch 14/25
 - 1s - loss: 0.7495 - acc: 0.7462 - val_loss: 0.7706 - val_acc: 0.7454
Epoch 15/25
 - 1s - loss: 0.7378 - acc: 0.7479 - val_loss: 0.7586 - val_acc: 0.7471
Epoch 16/25
 - 1s - loss: 0.7240 - acc: 0.7555 - val_loss: 0.7516 - val_acc: 0.7569
Epoch 17/25
 - 1s - loss: 0.7138 - acc: 0.7590 - val_loss: 0.7371 - val_acc: 0.7546
Epoch 18/25
 - 1s - loss: 0.7023 - acc: 0.7598 - val_loss: 0.7305 - val_acc: 0.7587
Epoch 19/25
 - 1s - loss: 0.6916 - acc: 0.7644 - val_loss: 0.7192 - val_acc: 0.7604
Epoch 20/25
 - 1s - loss: 0.6824 - acc: 0.7708 - val_loss: 0.7118 - val_acc: 0.7714
Epoch 21/25
 - 1s - loss: 0.6745 - acc: 0.7711 - val_loss: 0.7015 - val_acc: 0.7679
Epoch 22/25
 - 1s - loss: 0.6638 - acc: 0.7745 - val_loss: 0.6932 - val_acc: 0.7697
Epoch 23/25
 - 1s - loss: 0.6545 - acc: 0.7822 - val_loss: 0.6861 - val_acc: 0.7772
Epoch 24/25
 - 1s - loss: 0.6485 - acc: 0.7819 - val_loss: 0.6779 - val_acc: 0.7737
Epoch 25/25
 - 1s - loss: 0.6392 - acc: 0.7909 - val_loss: 0.6701 - val_acc: 0.7778
 - 0s - loss: 0.8513 - acc: 0.6286


Loss and metrics for TANH model with 64 batches and 25 epochs: 
Test Loss is 0.85 
Test Accuracy is 62.86 %

Train on 6474 samples, validate on 1728 samples
Epoch 1/40
 - 1s - loss: 1.2796 - acc: 0.4730 - val_loss: 1.1905 - val_acc: 0.5775
Epoch 2/40
 - 1s - loss: 1.1281 - acc: 0.5944 - val_loss: 1.0862 - val_acc: 0.6337
Epoch 3/40
 - 1s - loss: 1.0424 - acc: 0.6316 - val_loss: 1.0188 - val_acc: 0.6429
Epoch 4/40
 - 1s - loss: 0.9846 - acc: 0.6653 - val_loss: 0.9721 - val_acc: 0.6771
Epoch 5/40
 - 1s - loss: 0.9417 - acc: 0.6832 - val_loss: 0.9361 - val_acc: 0.6997
Epoch 6/40
 - 1s - loss: 0.9066 - acc: 0.6974 - val_loss: 0.9107 - val_acc: 0.6852
Epoch 7/40
 - 1s - loss: 0.8782 - acc: 0.7059 - val_loss: 0.8819 - val_acc: 0.7072
Epoch 8/40
 - 1s - loss: 0.8526 - acc: 0.7149 - val_loss: 0.8601 - val_acc: 0.7118
Epoch 9/40
 - 1s - loss: 0.8309 - acc: 0.7234 - val_loss: 0.8421 - val_acc: 0.7211
Epoch 10/40
 - 1s - loss: 0.8111 - acc: 0.7227 - val_loss: 0.8245 - val_acc: 0.7222
Epoch 11/40
 - 1s - loss: 0.7930 - acc: 0.7329 - val_loss: 0.8085 - val_acc: 0.7274
Epoch 12/40
 - 1s - loss: 0.7762 - acc: 0.7365 - val_loss: 0.7944 - val_acc: 0.7338
Epoch 13/40
 - 1s - loss: 0.7624 - acc: 0.7399 - val_loss: 0.7864 - val_acc: 0.7332
Epoch 14/40
 - 1s - loss: 0.7488 - acc: 0.7445 - val_loss: 0.7713 - val_acc: 0.7459
Epoch 15/40
 - 1s - loss: 0.7352 - acc: 0.7525 - val_loss: 0.7593 - val_acc: 0.7465
Epoch 16/40
 - 1s - loss: 0.7234 - acc: 0.7525 - val_loss: 0.7464 - val_acc: 0.7494
Epoch 17/40
 - 1s - loss: 0.7117 - acc: 0.7555 - val_loss: 0.7364 - val_acc: 0.7593
Epoch 18/40
 - 1s - loss: 0.7009 - acc: 0.7631 - val_loss: 0.7258 - val_acc: 0.7604
Epoch 19/40
 - 1s - loss: 0.6912 - acc: 0.7694 - val_loss: 0.7177 - val_acc: 0.7662
Epoch 20/40
 - 1s - loss: 0.6815 - acc: 0.7675 - val_loss: 0.7084 - val_acc: 0.7622
Epoch 21/40
 - 1s - loss: 0.6718 - acc: 0.7729 - val_loss: 0.7001 - val_acc: 0.7743
Epoch 22/40
 - 1s - loss: 0.6629 - acc: 0.7780 - val_loss: 0.6931 - val_acc: 0.7755
Epoch 23/40
 - 1s - loss: 0.6551 - acc: 0.7793 - val_loss: 0.6854 - val_acc: 0.7749
Epoch 24/40
 - 1s - loss: 0.6469 - acc: 0.7851 - val_loss: 0.6771 - val_acc: 0.7778
Epoch 25/40
 - 1s - loss: 0.6390 - acc: 0.7841 - val_loss: 0.6713 - val_acc: 0.7830
Epoch 26/40
 - 1s - loss: 0.6309 - acc: 0.7898 - val_loss: 0.6636 - val_acc: 0.7824
Epoch 27/40
 - 1s - loss: 0.6240 - acc: 0.7956 - val_loss: 0.6568 - val_acc: 0.7836
Epoch 28/40
 - 1s - loss: 0.6171 - acc: 0.7939 - val_loss: 0.6503 - val_acc: 0.7876
Epoch 29/40
 - 1s - loss: 0.6096 - acc: 0.7980 - val_loss: 0.6417 - val_acc: 0.7876
Epoch 30/40
 - 1s - loss: 0.6032 - acc: 0.8006 - val_loss: 0.6406 - val_acc: 0.7917
Epoch 31/40
 - 1s - loss: 0.5962 - acc: 0.8060 - val_loss: 0.6293 - val_acc: 0.7911
Epoch 32/40
 - 1s - loss: 0.5895 - acc: 0.8071 - val_loss: 0.6242 - val_acc: 0.7940
Epoch 33/40
 - 1s - loss: 0.5839 - acc: 0.8083 - val_loss: 0.6183 - val_acc: 0.7934
Epoch 34/40
 - 1s - loss: 0.5778 - acc: 0.8099 - val_loss: 0.6129 - val_acc: 0.7986
Epoch 35/40
 - 1s - loss: 0.5712 - acc: 0.8125 - val_loss: 0.6074 - val_acc: 0.7992
Epoch 36/40
 - 1s - loss: 0.5653 - acc: 0.8126 - val_loss: 0.6030 - val_acc: 0.7975
Epoch 37/40
 - 1s - loss: 0.5594 - acc: 0.8153 - val_loss: 0.5959 - val_acc: 0.8003
Epoch 38/40
 - 1s - loss: 0.5547 - acc: 0.8146 - val_loss: 0.5923 - val_acc: 0.7998
Epoch 39/40
 - 1s - loss: 0.5484 - acc: 0.8205 - val_loss: 0.5848 - val_acc: 0.8027
Epoch 40/40
 - 1s - loss: 0.5432 - acc: 0.8204 - val_loss: 0.5837 - val_acc: 0.8050
 - 0s - loss: 0.8428 - acc: 0.6457


Loss and metrics for TANH model with 64 batches and 40 epochs: 
Test Loss is 0.84 
Test Accuracy is 64.57 %

Train on 6474 samples, validate on 1728 samples
2021-02-24 22:36:00.997410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2021-02-24 22:36:00.997492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-02-24 22:36:00.997499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2021-02-24 22:36:00.997507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2021-02-24 22:36:00.997772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6361 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:5e:00.0, compute capability: 7.5)
Epoch 1/50
 - 1s - loss: 1.2176 - acc: 0.5263 - val_loss: 1.0817 - val_acc: 0.6209
Epoch 2/50
 - 1s - loss: 1.0263 - acc: 0.6423 - val_loss: 0.9751 - val_acc: 0.6562
Epoch 3/50
 - 1s - loss: 0.9452 - acc: 0.6906 - val_loss: 0.9130 - val_acc: 0.7135
Epoch 4/50
 - 1s - loss: 0.8870 - acc: 0.7238 - val_loss: 0.8631 - val_acc: 0.7344
Epoch 5/50
 - 1s - loss: 0.8362 - acc: 0.7451 - val_loss: 0.8194 - val_acc: 0.7541
Epoch 6/50
 - 1s - loss: 0.7937 - acc: 0.7606 - val_loss: 0.7794 - val_acc: 0.7679
Epoch 7/50
 - 1s - loss: 0.7557 - acc: 0.7739 - val_loss: 0.7501 - val_acc: 0.7743
Epoch 8/50
 - 1s - loss: 0.7235 - acc: 0.7875 - val_loss: 0.7179 - val_acc: 0.7807
Epoch 9/50
 - 1s - loss: 0.6937 - acc: 0.7960 - val_loss: 0.6903 - val_acc: 0.7946
Epoch 10/50
 - 1s - loss: 0.6678 - acc: 0.7994 - val_loss: 0.6667 - val_acc: 0.7946
Epoch 11/50
 - 1s - loss: 0.6427 - acc: 0.8065 - val_loss: 0.6466 - val_acc: 0.8021
Epoch 12/50
 - 1s - loss: 0.6210 - acc: 0.8142 - val_loss: 0.6235 - val_acc: 0.8096
Epoch 13/50
 - 1s - loss: 0.6003 - acc: 0.8207 - val_loss: 0.6059 - val_acc: 0.8166
Epoch 14/50
 - 1s - loss: 0.5820 - acc: 0.8250 - val_loss: 0.5904 - val_acc: 0.8194
Epoch 15/50
 - 1s - loss: 0.5645 - acc: 0.8302 - val_loss: 0.5751 - val_acc: 0.8247
Epoch 16/50
 - 1s - loss: 0.5496 - acc: 0.8332 - val_loss: 0.5580 - val_acc: 0.8339
Epoch 17/50
 - 1s - loss: 0.5338 - acc: 0.8380 - val_loss: 0.5467 - val_acc: 0.8322
Epoch 18/50
 - 1s - loss: 0.5202 - acc: 0.8401 - val_loss: 0.5366 - val_acc: 0.8339
Epoch 19/50
 - 1s - loss: 0.5074 - acc: 0.8449 - val_loss: 0.5235 - val_acc: 0.8385
Epoch 20/50
 - 1s - loss: 0.4952 - acc: 0.8462 - val_loss: 0.5094 - val_acc: 0.8472
Epoch 21/50
 - 1s - loss: 0.4842 - acc: 0.8494 - val_loss: 0.5026 - val_acc: 0.8455
Epoch 22/50
 - 1s - loss: 0.4732 - acc: 0.8543 - val_loss: 0.4916 - val_acc: 0.8455
Epoch 23/50
 - 1s - loss: 0.4642 - acc: 0.8551 - val_loss: 0.4823 - val_acc: 0.8478
Epoch 24/50
 - 1s - loss: 0.4540 - acc: 0.8599 - val_loss: 0.4747 - val_acc: 0.8524
Epoch 25/50
 - 1s - loss: 0.4459 - acc: 0.8593 - val_loss: 0.4654 - val_acc: 0.8536
Epoch 26/50
 - 1s - loss: 0.4376 - acc: 0.8641 - val_loss: 0.4564 - val_acc: 0.8571
Epoch 27/50
 - 1s - loss: 0.4298 - acc: 0.8655 - val_loss: 0.4517 - val_acc: 0.8594
Epoch 28/50
 - 1s - loss: 0.4217 - acc: 0.8679 - val_loss: 0.4460 - val_acc: 0.8594
Epoch 29/50
 - 1s - loss: 0.4152 - acc: 0.8706 - val_loss: 0.4457 - val_acc: 0.8669
Epoch 30/50
 - 1s - loss: 0.4080 - acc: 0.8693 - val_loss: 0.4351 - val_acc: 0.8634
Epoch 31/50
 - 1s - loss: 0.4015 - acc: 0.8726 - val_loss: 0.4270 - val_acc: 0.8692
Epoch 32/50
 - 1s - loss: 0.3952 - acc: 0.8746 - val_loss: 0.4208 - val_acc: 0.8698
Epoch 33/50
 - 1s - loss: 0.3904 - acc: 0.8764 - val_loss: 0.4145 - val_acc: 0.8704
Epoch 34/50
 - 1s - loss: 0.3839 - acc: 0.8792 - val_loss: 0.4095 - val_acc: 0.8744
Epoch 35/50
 - 1s - loss: 0.3788 - acc: 0.8794 - val_loss: 0.4073 - val_acc: 0.8738
Epoch 36/50
 - 1s - loss: 0.3729 - acc: 0.8817 - val_loss: 0.4041 - val_acc: 0.8848
Epoch 37/50
 - 1s - loss: 0.3693 - acc: 0.8800 - val_loss: 0.3973 - val_acc: 0.8773
Epoch 38/50
 - 1s - loss: 0.3634 - acc: 0.8829 - val_loss: 0.3941 - val_acc: 0.8831
Epoch 39/50
 - 1s - loss: 0.3591 - acc: 0.8851 - val_loss: 0.3907 - val_acc: 0.8831
Epoch 40/50
 - 1s - loss: 0.3547 - acc: 0.8863 - val_loss: 0.3880 - val_acc: 0.8802
Epoch 41/50
 - 1s - loss: 0.3504 - acc: 0.8872 - val_loss: 0.3869 - val_acc: 0.8802
Epoch 42/50
 - 1s - loss: 0.3467 - acc: 0.8888 - val_loss: 0.3821 - val_acc: 0.8843
Epoch 43/50
 - 1s - loss: 0.3420 - acc: 0.8909 - val_loss: 0.3760 - val_acc: 0.8854
Epoch 44/50
 - 1s - loss: 0.3387 - acc: 0.8914 - val_loss: 0.3714 - val_acc: 0.8895
Epoch 45/50
 - 1s - loss: 0.3348 - acc: 0.8931 - val_loss: 0.3673 - val_acc: 0.8872
Epoch 46/50
 - 1s - loss: 0.3321 - acc: 0.8937 - val_loss: 0.3645 - val_acc: 0.8866
Epoch 47/50
 - 1s - loss: 0.3281 - acc: 0.8945 - val_loss: 0.3621 - val_acc: 0.8872
Epoch 48/50
 - 1s - loss: 0.3255 - acc: 0.8945 - val_loss: 0.3604 - val_acc: 0.8872
Epoch 49/50
 - 1s - loss: 0.3214 - acc: 0.8971 - val_loss: 0.3567 - val_acc: 0.8935
Epoch 50/50
 - 1s - loss: 0.3188 - acc: 0.8977 - val_loss: 0.3534 - val_acc: 0.8935
 - 0s - loss: 0.8446 - acc: 0.6514


Loss and metrics for RELU model with 32 batches and 50 epochs: 
Test Loss is 0.84 
Test Accuracy is 65.14 %

Train on 6474 samples, validate on 1728 samples
2021-02-24 22:37:00.859516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2021-02-24 22:37:00.859624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-02-24 22:37:00.859631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2021-02-24 22:37:00.859640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2021-02-24 22:37:00.859841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6361 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:5e:00.0, compute capability: 7.5)
Epoch 1/50
 - 2s - loss: 1.0292 - acc: 0.5695 - val_loss: 0.9092 - val_acc: 0.6713
Epoch 2/50
 - 2s - loss: 0.7697 - acc: 0.7158 - val_loss: 0.7198 - val_acc: 0.7517
Epoch 3/50
 - 2s - loss: 0.6604 - acc: 0.7686 - val_loss: 0.6183 - val_acc: 0.8027
Epoch 4/50
 - 2s - loss: 0.5898 - acc: 0.7978 - val_loss: 0.5636 - val_acc: 0.8235
Epoch 5/50
 - 2s - loss: 0.5480 - acc: 0.8131 - val_loss: 0.5312 - val_acc: 0.8351
Epoch 6/50
 - 2s - loss: 0.5081 - acc: 0.8296 - val_loss: 0.4991 - val_acc: 0.8426
Epoch 7/50
 - 2s - loss: 0.4842 - acc: 0.8394 - val_loss: 0.4868 - val_acc: 0.8513
Epoch 8/50
 - 2s - loss: 0.4649 - acc: 0.8489 - val_loss: 0.4678 - val_acc: 0.8571
Epoch 9/50
 - 2s - loss: 0.4501 - acc: 0.8516 - val_loss: 0.4602 - val_acc: 0.8582
Epoch 10/50
 - 2s - loss: 0.4382 - acc: 0.8568 - val_loss: 0.4451 - val_acc: 0.8623
Epoch 11/50
 - 2s - loss: 0.4217 - acc: 0.8616 - val_loss: 0.4347 - val_acc: 0.8663
Epoch 12/50
 - 2s - loss: 0.4073 - acc: 0.8672 - val_loss: 0.4239 - val_acc: 0.8686
Epoch 13/50
 - 2s - loss: 0.3990 - acc: 0.8712 - val_loss: 0.4126 - val_acc: 0.8744
Epoch 14/50
 - 2s - loss: 0.3900 - acc: 0.8752 - val_loss: 0.4203 - val_acc: 0.8750
Epoch 15/50
 - 2s - loss: 0.3772 - acc: 0.8792 - val_loss: 0.4063 - val_acc: 0.8727
Epoch 16/50
 - 2s - loss: 0.3764 - acc: 0.8794 - val_loss: 0.3967 - val_acc: 0.8802
Epoch 17/50
 - 2s - loss: 0.3653 - acc: 0.8835 - val_loss: 0.3854 - val_acc: 0.8872
Epoch 18/50
 - 2s - loss: 0.3513 - acc: 0.8868 - val_loss: 0.3911 - val_acc: 0.8837
Epoch 19/50
 - 2s - loss: 0.3474 - acc: 0.8859 - val_loss: 0.3908 - val_acc: 0.8860
Epoch 20/50
 - 2s - loss: 0.3432 - acc: 0.8897 - val_loss: 0.3832 - val_acc: 0.8866
Epoch 21/50
 - 2s - loss: 0.3345 - acc: 0.8894 - val_loss: 0.3674 - val_acc: 0.8895
Epoch 22/50
 - 2s - loss: 0.3309 - acc: 0.8950 - val_loss: 0.3723 - val_acc: 0.8941
Epoch 23/50
 - 2s - loss: 0.3226 - acc: 0.8959 - val_loss: 0.3986 - val_acc: 0.8808
Epoch 24/50
 - 2s - loss: 0.3174 - acc: 0.8968 - val_loss: 0.3777 - val_acc: 0.8883
Epoch 25/50
 - 2s - loss: 0.3119 - acc: 0.8996 - val_loss: 0.3580 - val_acc: 0.8941
Epoch 26/50
 - 2s - loss: 0.3099 - acc: 0.9007 - val_loss: 0.3579 - val_acc: 0.8929
Epoch 27/50
 - 2s - loss: 0.2998 - acc: 0.9061 - val_loss: 0.3548 - val_acc: 0.8947
Epoch 28/50
 - 2s - loss: 0.2943 - acc: 0.9075 - val_loss: 0.3483 - val_acc: 0.8958
Epoch 29/50
 - 2s - loss: 0.3002 - acc: 0.9072 - val_loss: 0.3410 - val_acc: 0.9005
Epoch 30/50
 - 2s - loss: 0.2920 - acc: 0.9056 - val_loss: 0.3542 - val_acc: 0.8935
Epoch 31/50
 - 2s - loss: 0.2839 - acc: 0.9129 - val_loss: 0.3354 - val_acc: 0.9016
Epoch 32/50
 - 2s - loss: 0.2804 - acc: 0.9126 - val_loss: 0.3384 - val_acc: 0.9028
Epoch 33/50
 - 2s - loss: 0.2757 - acc: 0.9137 - val_loss: 0.3321 - val_acc: 0.9010
Epoch 34/50
 - 2s - loss: 0.2680 - acc: 0.9164 - val_loss: 0.3783 - val_acc: 0.8843
Epoch 35/50
 - 2s - loss: 0.2687 - acc: 0.9171 - val_loss: 0.3329 - val_acc: 0.9010
Epoch 36/50
 - 2s - loss: 0.2660 - acc: 0.9164 - val_loss: 0.3225 - val_acc: 0.9045
Epoch 37/50
 - 2s - loss: 0.2641 - acc: 0.9183 - val_loss: 0.3469 - val_acc: 0.8958
Epoch 38/50
 - 2s - loss: 0.2597 - acc: 0.9205 - val_loss: 0.3334 - val_acc: 0.9005
Epoch 39/50
 - 2s - loss: 0.2547 - acc: 0.9205 - val_loss: 0.3394 - val_acc: 0.8987
Epoch 40/50
 - 2s - loss: 0.2518 - acc: 0.9201 - val_loss: 0.3191 - val_acc: 0.9068
Epoch 41/50
 - 2s - loss: 0.2451 - acc: 0.9286 - val_loss: 0.3146 - val_acc: 0.9010
Epoch 42/50
 - 2s - loss: 0.2423 - acc: 0.9246 - val_loss: 0.3426 - val_acc: 0.8918
Epoch 43/50
 - 2s - loss: 0.2423 - acc: 0.9259 - val_loss: 0.3194 - val_acc: 0.9039
Epoch 44/50
 - 2s - loss: 0.2410 - acc: 0.9248 - val_loss: 0.3140 - val_acc: 0.9068
Epoch 45/50
 - 2s - loss: 0.2384 - acc: 0.9269 - val_loss: 0.3079 - val_acc: 0.9091
Epoch 46/50
 - 2s - loss: 0.2378 - acc: 0.9283 - val_loss: 0.3002 - val_acc: 0.9120
Epoch 47/50
 - 2s - loss: 0.2327 - acc: 0.9269 - val_loss: 0.3171 - val_acc: 0.9039
Epoch 48/50
 - 2s - loss: 0.2289 - acc: 0.9274 - val_loss: 0.3048 - val_acc: 0.9109
Epoch 49/50
 - 2s - loss: 0.2220 - acc: 0.9311 - val_loss: 0.3046 - val_acc: 0.9057
Epoch 50/50
 - 2s - loss: 0.2237 - acc: 0.9314 - val_loss: 0.3075 - val_acc: 0.9057
 - 0s - loss: 0.9649 - acc: 0.6229


Loss and metrics for TANH model with batch normalization, 32 batches and 50 epochs: 
Test Loss is 0.96 
Test Accuracy is 62.29 %

WARNING:tensorflow:From /opt/miniconda3/envs/2ndPaper-t2/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
permute (Permute)            (None, 32, 32, 1)         0         
_________________________________________________________________
Conv2D_1 (Conv2D)            (None, 32, 32, 64)        640       
_________________________________________________________________
dropout (Dropout)            (None, 32, 32, 64)        0         
_________________________________________________________________
batch_normalization_v1_2 (Ba (None, 32, 32, 64)        128       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         
_________________________________________________________________
Conv2D_2 (Conv2D)            (None, 16, 16, 128)       73856     
_________________________________________________________________
dropout_1 (Dropout)          (None, 16, 16, 128)       0         
_________________________________________________________________
batch_normalization_v1_3 (Ba (None, 16, 16, 128)       64        
_________________________________________________________________
Conv2D_3 (Conv2D)            (None, 16, 16, 128)       147584    
_________________________________________________________________
dropout_2 (Dropout)          (None, 16, 16, 128)       0         
_________________________________________________________________
batch_normalization_v1_4 (Ba (None, 16, 16, 128)       64        
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         
_________________________________________________________________
Conv2D_4 (Conv2D)            (None, 8, 8, 256)         295168    
_________________________________________________________________
dropout_3 (Dropout)          (None, 8, 8, 256)         0         
_________________________________________________________________
batch_normalization_v1_5 (Ba (None, 8, 8, 256)         32        
_________________________________________________________________
Conv2D_5 (Conv2D)            (None, 8, 8, 256)         590080    
_________________________________________________________________
dropout_4 (Dropout)          (None, 8, 8, 256)         0         
_________________________________________________________________
batch_normalization_v1_6 (Ba (None, 8, 8, 256)         32        
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4096)              0         
_________________________________________________________________
FCN_1 (Dense)                (None, 512)               2097664   
_________________________________________________________________
dropout_5 (Dropout)          (None, 512)               0         
_________________________________________________________________
FCN_2 (Dense)                (None, 128)               65664     
_________________________________________________________________
FCN_3 (Dense)                (None, 4)                 516       
=================================================================
Total params: 3,271,492
Trainable params: 3,271,332
Non-trainable params: 160
_________________________________________________________________
Train on 6474 samples, validate on 1728 samples
Epoch 1/25
6474/6474 [==============================] - 5s 717us/sample - loss: 8.0537 - acc: 0.4234 - val_loss: 7.8988 - val_acc: 0.2500
Epoch 2/25
6474/6474 [==============================] - 2s 324us/sample - loss: 7.6118 - acc: 0.5156 - val_loss: 8.0456 - val_acc: 0.2500
Epoch 3/25
6474/6474 [==============================] - 2s 326us/sample - loss: 7.4392 - acc: 0.5754 - val_loss: 8.0850 - val_acc: 0.2506
Epoch 4/25
6474/6474 [==============================] - 2s 333us/sample - loss: 7.3442 - acc: 0.6066 - val_loss: 8.1228 - val_acc: 0.2512
Epoch 5/25
6474/6474 [==============================] - 2s 325us/sample - loss: 7.2610 - acc: 0.6236 - val_loss: 8.0626 - val_acc: 0.2743
Epoch 6/25
6474/6474 [==============================] - 2s 325us/sample - loss: 7.1900 - acc: 0.6463 - val_loss: 7.9179 - val_acc: 0.3125
Epoch 7/25
6474/6474 [==============================] - 2s 325us/sample - loss: 7.1148 - acc: 0.6603 - val_loss: 7.7469 - val_acc: 0.4074
Epoch 8/25
6474/6474 [==============================] - 2s 325us/sample - loss: 7.0834 - acc: 0.6773 - val_loss: 7.6728 - val_acc: 0.4294
Epoch 9/25
6474/6474 [==============================] - 2s 323us/sample - loss: 7.0241 - acc: 0.6838 - val_loss: 7.6388 - val_acc: 0.4479
Epoch 10/25
6474/6474 [==============================] - 2s 334us/sample - loss: 6.9957 - acc: 0.6943 - val_loss: 7.6098 - val_acc: 0.4601
Epoch 11/25
6474/6474 [==============================] - 2s 329us/sample - loss: 6.9359 - acc: 0.7091 - val_loss: 7.5806 - val_acc: 0.4670
Epoch 12/25
6474/6474 [==============================] - 2s 327us/sample - loss: 6.8894 - acc: 0.7281 - val_loss: 7.5560 - val_acc: 0.4387
Epoch 13/25
6474/6474 [==============================] - 2s 328us/sample - loss: 6.8572 - acc: 0.7302 - val_loss: 7.5523 - val_acc: 0.4641
Epoch 14/25
6474/6474 [==============================] - 2s 330us/sample - loss: 6.8249 - acc: 0.7377 - val_loss: 7.5432 - val_acc: 0.4606
Epoch 15/25
6474/6474 [==============================] - 2s 318us/sample - loss: 6.7881 - acc: 0.7407 - val_loss: 7.5375 - val_acc: 0.4473
Epoch 16/25
6474/6474 [==============================] - 2s 321us/sample - loss: 6.7545 - acc: 0.7481 - val_loss: 7.4889 - val_acc: 0.4485
Epoch 17/25
6474/6474 [==============================] - 2s 327us/sample - loss: 6.7223 - acc: 0.7555 - val_loss: 7.4859 - val_acc: 0.4560
Epoch 18/25
6474/6474 [==============================] - 2s 322us/sample - loss: 6.6978 - acc: 0.7606 - val_loss: 7.4866 - val_acc: 0.4572
Epoch 19/25
6474/6474 [==============================] - 2s 325us/sample - loss: 6.6708 - acc: 0.7681 - val_loss: 7.4830 - val_acc: 0.4433
Epoch 20/25
6474/6474 [==============================] - 2s 324us/sample - loss: 6.6318 - acc: 0.7762 - val_loss: 7.4509 - val_acc: 0.4653
Epoch 21/25
6474/6474 [==============================] - 2s 320us/sample - loss: 6.6192 - acc: 0.7765 - val_loss: 7.4349 - val_acc: 0.4763
Epoch 22/25
6474/6474 [==============================] - 2s 324us/sample - loss: 6.5788 - acc: 0.7876 - val_loss: 7.4243 - val_acc: 0.4670
Epoch 23/25
6474/6474 [==============================] - 2s 325us/sample - loss: 6.5632 - acc: 0.7895 - val_loss: 7.4016 - val_acc: 0.4740
Epoch 24/25
6474/6474 [==============================] - 2s 317us/sample - loss: 6.5362 - acc: 0.7964 - val_loss: 7.3954 - val_acc: 0.4797
Epoch 25/25
6474/6474 [==============================] - 2s 317us/sample - loss: 6.5159 - acc: 0.7984 - val_loss: 7.3969 - val_acc: 0.4699
175/175 [==============================] - 0s 437us/sample - loss: 7.9014 - acc: 0.3314
test loss, test acc: [7.901441225324358, 0.33142856]
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
permute_1 (Permute)          (None, 32, 32, 1)         0         
_________________________________________________________________
Conv2D_1 (Conv2D)            (None, 32, 32, 32)        320       
_________________________________________________________________
dropout_6 (Dropout)          (None, 32, 32, 32)        0         
_________________________________________________________________
batch_normalization_v1_7 (Ba (None, 32, 32, 32)        128       
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 16, 16, 32)        0         
_________________________________________________________________
Conv2D_2 (Conv2D)            (None, 16, 16, 64)        18496     
_________________________________________________________________
dropout_7 (Dropout)          (None, 16, 16, 64)        0         
_________________________________________________________________
batch_normalization_v1_8 (Ba (None, 16, 16, 64)        64        
_________________________________________________________________
Conv2D_3 (Conv2D)            (None, 16, 16, 64)        36928     
_________________________________________________________________
dropout_8 (Dropout)          (None, 16, 16, 64)        0         
_________________________________________________________________
batch_normalization_v1_9 (Ba (None, 16, 16, 64)        64        
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 8, 8, 64)          0         
_________________________________________________________________
Conv2D_4 (Conv2D)            (None, 8, 8, 128)         73856     
_________________________________________________________________
dropout_9 (Dropout)          (None, 8, 8, 128)         0         
_________________________________________________________________
batch_normalization_v1_10 (B (None, 8, 8, 128)         32        
_________________________________________________________________
Conv2D_5 (Conv2D)            (None, 8, 8, 128)         147584    
_________________________________________________________________
dropout_10 (Dropout)         (None, 8, 8, 128)         0         
_________________________________________________________________
batch_normalization_v1_11 (B (None, 8, 8, 128)         32        
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 4, 4, 128)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 2048)              0         
_________________________________________________________________
FCN_1 (Dense)                (None, 512)               1049088   
_________________________________________________________________
dropout_11 (Dropout)         (None, 512)               0         
_________________________________________________________________
FCN_2 (Dense)                (None, 128)               65664     
_________________________________________________________________
FCN_3 (Dense)                (None, 4)                 516       
=================================================================
Total params: 1,392,772
Trainable params: 1,392,612
Non-trainable params: 160
_________________________________________________________________
Train on 6474 samples, validate on 1728 samples
Epoch 1/25
6474/6474 [==============================] - 3s 420us/sample - loss: 4.7604 - acc: 0.4428 - val_loss: 4.6868 - val_acc: 0.2500
Epoch 2/25
6474/6474 [==============================] - 2s 261us/sample - loss: 4.5249 - acc: 0.5178 - val_loss: 4.7934 - val_acc: 0.2500
Epoch 3/25
6474/6474 [==============================] - 2s 258us/sample - loss: 4.4142 - acc: 0.5391 - val_loss: 4.8020 - val_acc: 0.2517
Epoch 4/25
6474/6474 [==============================] - 2s 264us/sample - loss: 4.3582 - acc: 0.5477 - val_loss: 4.7372 - val_acc: 0.2569
Epoch 5/25
6474/6474 [==============================] - 2s 256us/sample - loss: 4.2987 - acc: 0.5743 - val_loss: 4.6198 - val_acc: 0.2755
Epoch 6/25
6474/6474 [==============================] - 2s 258us/sample - loss: 4.2663 - acc: 0.5808 - val_loss: 4.5192 - val_acc: 0.3304
Epoch 7/25
6474/6474 [==============================] - 2s 259us/sample - loss: 4.2386 - acc: 0.5942 - val_loss: 4.4641 - val_acc: 0.3704
Epoch 8/25
6474/6474 [==============================] - 2s 269us/sample - loss: 4.2101 - acc: 0.5961 - val_loss: 4.4434 - val_acc: 0.3958
Epoch 9/25
6474/6474 [==============================] - 2s 263us/sample - loss: 4.1790 - acc: 0.6044 - val_loss: 4.4344 - val_acc: 0.4080
Epoch 10/25
6474/6474 [==============================] - 2s 264us/sample - loss: 4.1744 - acc: 0.6081 - val_loss: 4.4242 - val_acc: 0.4057
Epoch 11/25
6474/6474 [==============================] - 2s 255us/sample - loss: 4.1475 - acc: 0.6202 - val_loss: 4.4191 - val_acc: 0.4109
Epoch 12/25
6474/6474 [==============================] - 2s 258us/sample - loss: 4.1291 - acc: 0.6165 - val_loss: 4.4132 - val_acc: 0.4172
Epoch 13/25
6474/6474 [==============================] - 2s 261us/sample - loss: 4.1186 - acc: 0.6182 - val_loss: 4.4092 - val_acc: 0.4115
Epoch 14/25
6474/6474 [==============================] - 2s 266us/sample - loss: 4.0996 - acc: 0.6274 - val_loss: 4.4054 - val_acc: 0.4161
Epoch 15/25
6474/6474 [==============================] - 2s 263us/sample - loss: 4.0639 - acc: 0.6395 - val_loss: 4.4038 - val_acc: 0.4161
Epoch 16/25
6474/6474 [==============================] - 2s 256us/sample - loss: 4.0588 - acc: 0.6491 - val_loss: 4.3987 - val_acc: 0.4167
Epoch 17/25
6474/6474 [==============================] - 2s 252us/sample - loss: 4.0452 - acc: 0.6440 - val_loss: 4.3957 - val_acc: 0.4196
Epoch 18/25
6474/6474 [==============================] - 2s 254us/sample - loss: 4.0264 - acc: 0.6515 - val_loss: 4.3943 - val_acc: 0.4190
Epoch 19/25
6474/6474 [==============================] - 2s 256us/sample - loss: 4.0342 - acc: 0.6480 - val_loss: 4.3989 - val_acc: 0.4207
Epoch 20/25
6474/6474 [==============================] - 2s 254us/sample - loss: 4.0023 - acc: 0.6636 - val_loss: 4.3975 - val_acc: 0.4219
Epoch 21/25
6474/6474 [==============================] - 2s 258us/sample - loss: 3.9932 - acc: 0.6645 - val_loss: 4.3949 - val_acc: 0.4236
Epoch 22/25
6474/6474 [==============================] - 2s 249us/sample - loss: 3.9890 - acc: 0.6667 - val_loss: 4.3984 - val_acc: 0.4253
Epoch 23/25
6474/6474 [==============================] - 2s 246us/sample - loss: 3.9733 - acc: 0.6696 - val_loss: 4.3997 - val_acc: 0.4230
Epoch 24/25
6474/6474 [==============================] - 2s 258us/sample - loss: 3.9739 - acc: 0.6736 - val_loss: 4.4031 - val_acc: 0.4213
Epoch 25/25
6474/6474 [==============================] - 2s 255us/sample - loss: 3.9541 - acc: 0.6747 - val_loss: 4.3998 - val_acc: 0.4230
175/175 [==============================] - 0s 273us/sample - loss: 4.7209 - acc: 0.3029
test loss, test acc: [4.720892764500209, 0.30285713]

All the results we got are:

                  RELU (64 batches, 25 epochs)  TANH (64 batches, 25 epochs)  \
Test loss                             0.804508                      0.851335   
Test accuracy(%)                     66.857141                     62.857145   

                  TANH (64 batches, 40 epochs)  RELU (32 batches, 50 epochs)  \
Test loss                             0.842778                      0.844596   
Test accuracy(%)                     64.571428                     65.142858   

                  TANH (batch normalized, 32 batches, 25 epochs)  \
Test loss                                               0.964857   
Test accuracy(%)                                       62.285715   

                  NNet ([64,128,128,256,256] filters)  \
Test loss                                    7.901441   
Test accuracy(%)                            33.142856   

                  NNet ([32,64,64,128,128] filters)  
Test loss                                  4.720893  
Test accuracy(%)                          30.285713  


(2ndPaper-t2) stu4@triton01:~/HW4_yoel_amit$ 
